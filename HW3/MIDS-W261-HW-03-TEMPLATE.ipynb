{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATSCIW261 ASSIGNMENT \n",
    "Version 2016-01-27 (FINAL)\n",
    "Week 3 ASSIGNMENTS\n",
    "\n",
    "---\n",
    "Link: https://docs.google.com/spreadsheets/d/1ncFQl5Tovn-16slD8mYjP_nzMTPSfiGeLLzW8v_sMjg/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be extra meta, here's the NBViewer for this assignment\n",
    "\n",
    "http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/zt176cf7u0y0nqs/MIDS-W261-HW-03-TEMPLATE.ipynb\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "1. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "1. What is  a combiner function in the context of Hadoop? \n",
    "1. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "1. What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sudo touch mapper31.py\n",
    "sudo touch reducer31.py\n",
    "chmod -R 777 /home/cloudera/share/HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper31.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:MapperCounters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if \"debt\" in line:\n",
    "        sys.stderr.write(\"reporter:counter:DebtCounter,Total,1\\n\")\n",
    "    elif \"mortgage\" in line:\n",
    "        sys.stderr.write(\"reporter:counter:MortgageCounter,Total,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:OtherCounter,Total,1\\n\")        \n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer31.py\n",
    "#!/usr/bin/env python\n",
    "import sys \n",
    "\n",
    "sys.stderr.write(\"reporter:counter:ReducerCounters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Consumer_complaints.csv\n",
      "Deleted consumer-complaints\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1722261065283592618.jar tmpDir=null\n",
      "16/10/18 14:57:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/10/18 14:57:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/10/18 14:57:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/10/18 14:57:15 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/10/18 14:57:15 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/10/18 14:57:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/10/18 14:57:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1476801081955_0012\n",
      "16/10/18 14:57:16 INFO impl.YarnClientImpl: Submitted application application_1476801081955_0012\n",
      "16/10/18 14:57:16 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1476801081955_0012/\n",
      "16/10/18 14:57:16 INFO mapreduce.Job: Running job: job_1476801081955_0012\n",
      "16/10/18 14:57:26 INFO mapreduce.Job: Job job_1476801081955_0012 running in uber mode : false\n",
      "16/10/18 14:57:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/10/18 14:57:43 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/10/18 14:57:44 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/10/18 14:57:47 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/10/18 14:57:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/10/18 14:58:12 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/10/18 14:58:13 INFO mapreduce.Job:  map 100% reduce 48%\n",
      "16/10/18 14:58:15 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/10/18 14:58:18 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/10/18 14:58:19 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/10/18 14:58:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/10/18 14:58:20 INFO mapreduce.Job: Job job_1476801081955_0012 completed successfully\n",
      "16/10/18 14:58:20 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=56163339\n",
      "\t\tFILE: Number of bytes written=112921407\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910824\n",
      "\t\tHDFS: Number of bytes written=55600181\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38319\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=82259\n",
      "\t\tTotal time spent by all map tasks (ms)=38319\n",
      "\t\tTotal time spent by all reduce tasks (ms)=82259\n",
      "\t\tTotal vcore-seconds taken by all map tasks=38319\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=82259\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=39238656\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=84233216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=625826\n",
      "\t\tMap output bytes=52126201\n",
      "\t\tMap output materialized bytes=56163357\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=625826\n",
      "\t\tCombine output records=1251652\n",
      "\t\tReduce input groups=625827\n",
      "\t\tReduce shuffle bytes=56163357\n",
      "\t\tReduce input records=1251652\n",
      "\t\tReduce output records=2503304\n",
      "\t\tSpilled Records=2503304\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=881\n",
      "\t\tCPU time spent (ms)=20180\n",
      "\t\tPhysical memory (bytes) snapshot=877768704\n",
      "\t\tVirtual memory (bytes) snapshot=7529910272\n",
      "\t\tTotal committed heap usage (bytes)=513482752\n",
      "\tDebtCounter\n",
      "\t\tTotal=29092\n",
      "\tMapperCounters\n",
      "\t\tCalls=2\n",
      "\tMortgageCounter\n",
      "\t\tTotal=120917\n",
      "\tOtherCounter\n",
      "\t\tTotal=162904\n",
      "\tReducerCounters\n",
      "\t\tCalls=9\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=55600181\n",
      "16/10/18 14:58:20 INFO streaming.StreamJob: Output directory: consumer-complaints\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm Consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal /home/cloudera/share/HW3/Consumer_complaints.csv \n",
    "!hdfs dfs -rm -r consumer-complaints\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/home/cloudera/share/HW3\"\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -mapper /home/cloudera/share/HW3/mapper31.py \\\n",
    "   -reducer /home/cloudera/share/HW3/reducer31.py \\\n",
    "   -combiner /home/cloudera/share/HW3/reducer31.py \\\n",
    "   -input Consumer_complaints.csv \\\n",
    "   -output consumer-complaints \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2  depecated\n",
    "#-input historical_tours.txt  file on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CounterScreenshots.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo touch generic_word_count.py\n",
    "chmod -R 777 /home/cloudera/share/HW3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generic_word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generic_word_count.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#Mapper and reducer with mrjob just counting calls to each component, just using single line text\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "wordRe = re.compile(r\"[\\w]+\")\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('group','num_mapper_calls',1)\n",
    "        for word in wordRe.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        self.increment_counter('group','num_reducer_calls',1)        \n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/generic_word_count.cloudera.20161018.185904.663591\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/generic_word_count.cloudera.20161018.185904.663591/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4120978477659966749.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "  Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1476801081955_0013\n",
      "  Submitted application application_1476801081955_0013\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1476801081955_0013/\n",
      "  Running job: job_1476801081955_0013\n",
      "  Job job_1476801081955_0013 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1476801081955_0013 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/generic_word_count.cloudera.20161018.185904.663591/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=47\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=79\n",
      "\t\tFILE: Number of bytes written=365557\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=399\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=20408320\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5341184\n",
      "\t\tTotal time spent by all map tasks (ms)=19930\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19930\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5216\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5216\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19930\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5216\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1890\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=249\n",
      "\t\tInput split bytes=352\n",
      "\t\tMap input records=1\n",
      "\t\tMap output bytes=59\n",
      "\t\tMap output materialized bytes=85\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=572551168\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=85\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=391979008\n",
      "\t\tVirtual memory (bytes) snapshot=4511346688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tnum_mapper_calls=1\n",
      "\t\tnum_reducer_calls=4\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/generic_word_count.cloudera.20161018.185904.663591/output...\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/generic_word_count.cloudera.20161018.185904.663591...\n",
      "Removing temp directory /tmp/generic_word_count.cloudera.20161018.185904.663591...\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" > oneLine.txt\n",
    "!chmod 777 oneLine.txt\n",
    "!python generic_word_count.py -r hadoop oneLine.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mapper has been called one time while the reducer has been called four times.  This makes sense because the one-line file is so sort that it only requires a single mapper, and there are four unique words, or keys, so each one will  get shuffled to a separate reducer, thus giving us four reducer calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaints_word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaints_word_count.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#Same as above but we run on complaints data and isolate the issue column\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "wordRe = re.compile(r\"[\\w]+\")\n",
    "\n",
    "class MRComplaintFrequencyCount(MRJob):\n",
    "\n",
    "    MRJob.SORT_VALUES = True   \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('group','num_mapper_calls',1)\n",
    "        \n",
    "        #Issue is third column in csv\n",
    "        issue = line.split(\",\")[3]\n",
    "        \n",
    "        for word in wordRe.findall(issue):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        self.increment_counter('group','num_reducer_calls',1)        \n",
    "        yield key, sum(values)\n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.fields': 2,\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "        }\n",
    "        return [MRStep(jobconf=JOBCONF_STEP,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer)\n",
    "                ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRComplaintFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted complaints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/complaints_word_count.cloudera.20161018.203425.888410\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/complaints_word_count.cloudera.20161018.203425.888410/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4894672050164590965.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1476801081955_0023\n",
      "  Submitted application application_1476801081955_0023\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1476801081955_0023/\n",
      "  Running job: job_1476801081955_0023\n",
      "  Job job_1476801081955_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1476801081955_0023 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/complaints\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21109\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4973\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6341\n",
      "\t\tFILE: Number of bytes written=380592\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21483\n",
      "\t\tHDFS: Number of bytes written=4973\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=34453504\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=13476864\n",
      "\t\tTotal time spent by all map tasks (ms)=33646\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33646\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13161\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13161\n",
      "\t\tTotal vcore-seconds taken by all map tasks=33646\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=13161\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2320\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=527\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=5427\n",
      "\t\tMap output materialized bytes=6347\n",
      "\t\tMap output records=454\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=527671296\n",
      "\t\tReduce input groups=454\n",
      "\t\tReduce input records=454\n",
      "\t\tReduce output records=454\n",
      "\t\tReduce shuffle bytes=6347\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=908\n",
      "\t\tTotal committed heap usage (bytes)=391979008\n",
      "\t\tVirtual memory (bytes) snapshot=4511825920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tnum_mapper_calls=100\n",
      "\t\tnum_reducer_calls=454\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/complaints_word_count.cloudera.20161018.203425.888410...\n",
      "Removing temp directory /tmp/complaints_word_count.cloudera.20161018.203425.888410...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r complaints\n",
    "python complaints_word_count.py -r hadoop TinyComplaints.csv --output-dir complaints --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'num_mapper_calls': 312913, 'num_reducer_calls': 172}}]\n"
     ]
    }
   ],
   "source": [
    "from complaints_word_count import MRComplaintFrequencyCount\n",
    "\n",
    "mrJob = MRComplaintFrequencyCount(args=[\"Consumer_complaints.csv\"])\n",
    "\n",
    "with mrJob.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaints_word_count_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaints_word_count_combiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#Same as above but we run on complaints data and isolate the issue column\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "wordRe = re.compile(r\"[\\w]+\")\n",
    "\n",
    "class MRComplaintFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('group','num_mapper_calls',1)\n",
    "        \n",
    "        #Issue is third column in csv\n",
    "        issue = line.split(\",\")[3]\n",
    "        \n",
    "        for word in wordRe.findall(issue):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        self.increment_counter('group','num_reducer_calls',1)        \n",
    "        yield key, sum(values)\n",
    "    \n",
    "    def combiner(self, key, values):\n",
    "        self.increment_counter('group','num_combiner_calls',1)        \n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRComplaintFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t3503\n",
      "\"account\"\t20681\n",
      "\"acct\"\t163\n",
      "\"action\"\t2505\n",
      "\"advance\"\t240\n",
      "\"advertising\"\t1193\n",
      "\"amount\"\t98\n",
      "\"amt\"\t71\n",
      "\"an\"\t2505\n",
      "\"and\"\t16448\n",
      "\"application\"\t8868\n",
      "\"applied\"\t139\n",
      "\"apply\"\t118\n",
      "\"apr\"\t3431\n",
      "\"arbitration\"\t168\n",
      "\"are\"\t3821\n",
      "\"atm\"\t2422\n",
      "\"attempts\"\t11848\n",
      "\"available\"\t274\n",
      "\"balance\"\t597\n",
      "\"bank\"\t202\n",
      "\"bankruptcy\"\t222\n",
      "\"being\"\t5663\n",
      "\"billing\"\t8158\n",
      "\"by\"\t5663\n",
      "\"can\"\t1999\n",
      "\"cancelling\"\t2795\n",
      "\"card\"\t4405\n",
      "\"cash\"\t240\n",
      "\"caused\"\t5663\n",
      "\"changes\"\t350\n",
      "\"charged\"\t976\n",
      "\"charges\"\t131\n",
      "\"checks\"\t75\n",
      "\"closing\"\t2795\n",
      "\"club\"\t12545\n",
      "\"collect\"\t11848\n",
      "\"collection\"\t1907\n",
      "\"communication\"\t6920\n",
      "\"company\"\t4858\n",
      "\"cont\"\t11848\n",
      "\"contact\"\t3053\n",
      "\"convenience\"\t75\n",
      "\"costs\"\t4350\n",
      "\"credit\"\t55251\n",
      "\"credited\"\t92\n",
      "\"customer\"\t2734\n",
      "\"d\"\t11848\n",
      "\"day\"\t71\n",
      "\"dealing\"\t1944\n",
      "\"debit\"\t2422\n",
      "\"debt\"\t19309\n",
      "\"decision\"\t2774\n",
      "\"decrease\"\t1149\n",
      "\"delay\"\t243\n",
      "\"delinquent\"\t1061\n",
      "\"deposits\"\t10555\n",
      "\"determination\"\t1490\n",
      "\"did\"\t139\n",
      "\"didn\"\t925\n",
      "\"disclosure\"\t5214\n",
      "\"disclosures\"\t64\n",
      "\"dispute\"\t904\n",
      "\"disputes\"\t6938\n",
      "\"embezzlement\"\t3276\n",
      "\"expect\"\t807\n",
      "\"false\"\t2508\n",
      "\"fee\"\t3198\n",
      "\"fees\"\t807\n",
      "\"for\"\t929\n",
      "\"forbearance\"\t350\n",
      "\"fraud\"\t3842\n",
      "\"funds\"\t5663\n",
      "\"get\"\t4357\n",
      "\"getting\"\t291\n",
      "\"health\"\t12545\n",
      "\"i\"\t925\n",
      "\"identity\"\t4729\n",
      "\"illegal\"\t2505\n",
      "\"improper\"\t4309\n",
      "\"incorrect\"\t29133\n",
      "\"increase\"\t1149\n",
      "\"info\"\t2896\n",
      "\"information\"\t29069\n",
      "\"interest\"\t4238\n",
      "\"investigation\"\t4858\n",
      "\"issuance\"\t640\n",
      "\"issue\"\t1099\n",
      "\"issues\"\t538\n",
      "\"late\"\t1797\n",
      "\"lease\"\t6337\n",
      "\"lender\"\t2165\n",
      "\"line\"\t1732\n",
      "\"loan\"\t119630\n",
      "\"low\"\t5663\n",
      "\"making\"\t3226\n",
      "\"managing\"\t5006\n",
      "\"marketing\"\t1193\n",
      "\"missing\"\t64\n",
      "\"modification\"\t70487\n",
      "\"money\"\t413\n",
      "\"monitoring\"\t1453\n",
      "\"my\"\t10731\n",
      "\"not\"\t12353\n",
      "\"of\"\t10885\n",
      "\"on\"\t29069\n",
      "\"opening\"\t16205\n",
      "\"or\"\t22533\n",
      "\"other\"\t7886\n",
      "\"out\"\t1242\n",
      "\"overlimit\"\t127\n",
      "\"owed\"\t11848\n",
      "\"pay\"\t3821\n",
      "\"payment\"\t92\n",
      "\"payments\"\t3226\n",
      "\"payoff\"\t1155\n",
      "\"plans\"\t350\n",
      "\"practices\"\t1003\n",
      "\"privacy\"\t240\n",
      "\"problems\"\t9484\n",
      "\"process\"\t5505\n",
      "\"processing\"\t243\n",
      "\"promised\"\t274\n",
      "\"protection\"\t4139\n",
      "\"rate\"\t3431\n",
      "\"receive\"\t139\n",
      "\"received\"\t216\n",
      "\"receiving\"\t3226\n",
      "\"relations\"\t1367\n",
      "\"repay\"\t1647\n",
      "\"repaying\"\t3844\n",
      "\"report\"\t34903\n",
      "\"reporting\"\t6559\n",
      "\"representation\"\t2508\n",
      "\"rewards\"\t1002\n",
      "\"s\"\t4858\n",
      "\"sale\"\t139\n",
      "\"scam\"\t566\n",
      "\"score\"\t4357\n",
      "\"service\"\t1518\n",
      "\"servicer\"\t1944\n",
      "\"servicing\"\t36767\n",
      "\"settlement\"\t4350\n",
      "\"sharing\"\t2832\n",
      "\"shopping\"\t672\n",
      "\"statement\"\t1220\n",
      "\"statements\"\t2508\n",
      "\"stop\"\t131\n",
      "\"t\"\t2924\n",
      "\"tactics\"\t6920\n",
      "\"taking\"\t3747\n",
      "\"terms\"\t350\n",
      "\"the\"\t6248\n",
      "\"theft\"\t3276\n",
      "\"threatening\"\t2505\n",
      "\"to\"\t8401\n",
      "\"transaction\"\t1485\n",
      "\"transfer\"\t597\n",
      "\"unable\"\t8178\n",
      "\"underwriting\"\t2774\n",
      "\"unsolicited\"\t640\n",
      "\"use\"\t1477\n",
      "\"using\"\t2422\n",
      "\"verification\"\t5214\n",
      "\"was\"\t274\n",
      "\"when\"\t4095\n",
      "\"with\"\t1944\n",
      "\"withdrawals\"\t10555\n",
      "\"workout\"\t350\n",
      "\"wrong\"\t169\n",
      "\"you\"\t3821\n",
      "\"your\"\t3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/complaints_word_count_combiner.cloudera.20160920.135940.261521\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tnum_combiner_calls=318\n",
      "\t\tnum_mapper_calls=312913\n",
      "Counters: 3\n",
      "\tgroup\n",
      "\t\tnum_combiner_calls=318\n",
      "\t\tnum_mapper_calls=312913\n",
      "\t\tnum_reducer_calls=172\n",
      "Streaming final output from complaints...\n",
      "Removing temp directory /tmp/complaints_word_count_combiner.cloudera.20160920.135940.261521...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python complaints_word_count_combiner.py Consumer_complaints.csv --output-dir complaints\n",
    "sudo chmod -R 777 complaints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"identity\"\t4729\n",
      "\"company\"\t4858\n",
      "\"investigation\"\t4858\n",
      "\"s\"\t4858\n",
      "\"managing\"\t5006\n",
      "\"disclosure\"\t5214\n",
      "\"verification\"\t5214\n",
      "\"process\"\t5505\n",
      "\"being\"\t5663\n",
      "\"by\"\t5663\n",
      "\"caused\"\t5663\n",
      "\"funds\"\t5663\n",
      "\"low\"\t5663\n",
      "\"the\"\t6248\n",
      "\"lease\"\t6337\n",
      "\"reporting\"\t6559\n",
      "\"communication\"\t6920\n",
      "\"tactics\"\t6920\n",
      "\"disputes\"\t6938\n",
      "\"other\"\t7886\n",
      "\"billing\"\t8158\n",
      "\"unable\"\t8178\n",
      "\"to\"\t8401\n",
      "\"application\"\t8868\n",
      "\"problems\"\t9484\n",
      "\"deposits\"\t10555\n",
      "\"withdrawals\"\t10555\n",
      "\"my\"\t10731\n",
      "\"of\"\t10885\n",
      "\"attempts\"\t11848\n",
      "\"collect\"\t11848\n",
      "\"cont\"\t11848\n",
      "\"d\"\t11848\n",
      "\"owed\"\t11848\n",
      "\"not\"\t12353\n",
      "\"club\"\t12545\n",
      "\"health\"\t12545\n",
      "\"opening\"\t16205\n",
      "\"and\"\t16448\n",
      "\"debt\"\t19309\n",
      "\"account\"\t20681\n",
      "\"or\"\t22533\n",
      "\"information\"\t29069\n",
      "\"on\"\t29069\n",
      "\"incorrect\"\t29133\n",
      "\"report\"\t34903\n",
      "\"servicing\"\t36767\n",
      "\"credit\"\t55251\n",
      "\"modification\"\t70487\n",
      "\"loan\"\t119630\n",
      "\n",
      "\"disclosures\"\t64\n",
      "\"missing\"\t64\n",
      "\"amt\"\t71\n",
      "\"day\"\t71\n",
      "\"checks\"\t75\n",
      "\"convenience\"\t75\n",
      "\"credited\"\t92\n",
      "\"payment\"\t92\n",
      "\"amount\"\t98\n",
      "\"apply\"\t118\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat complaints/* | sort -k2,2 -g | tail -50\n",
    "printf \"\\n\"\n",
    "cat complaints/* | sort -k2,2 -g | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaints_word_count_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaints_word_count_combiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#Same as above but we run on complaints data and isolate the issue column\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "wordRe = re.compile(r\"[\\w]+\")\n",
    "\n",
    "class MRComplaintFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('group','num_mapper_calls',1)\n",
    "        \n",
    "        #Issue is third column in csv\n",
    "        issue = line.split(\",\")[3]\n",
    "        \n",
    "        for word in wordRe.findall(issue):\n",
    "            #Send all map outputs to same reducer\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        self.increment_counter('group','num_reducer_calls',1)  \n",
    "        wordCounts = defaultdict(int)\n",
    "        total = 0         \n",
    "        for value in values:\n",
    "            word, count = value\n",
    "            total+=count\n",
    "            wordCounts[word]+=count\n",
    "            \n",
    "        for k,v in wordCounts.iteritems():\n",
    "            yield k, (v, float(v)/total)\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        self.increment_counter('group','num_combiner_calls',1) \n",
    "        yield None, (key, sum(values))\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRComplaintFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"unsolicited\"\t[640, 0.0006399276881712367]\n",
      "\"being\"\t[5663, 0.005662360153302677]\n",
      "\"caused\"\t[5663, 0.005662360153302677]\n",
      "\"scam\"\t[566, 0.0005659360492264374]\n",
      "\"embezzlement\"\t[3276, 0.0032756298538265177]\n",
      "\"report\"\t[34903, 0.03489905640662605]\n",
      "\"attempts\"\t[11848, 0.011846661327270018]\n",
      "\"settlement\"\t[4350, 0.004349508505538874]\n",
      "\"underwriting\"\t[2774, 0.002773686573417204]\n",
      "\"issues\"\t[538, 0.0005379392128689459]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/complaints_word_count_combiner.cloudera.20160920.175242.965136\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tnum_combiner_calls=318\n",
      "\t\tnum_mapper_calls=312913\n",
      "Counters: 3\n",
      "\tgroup\n",
      "\t\tnum_combiner_calls=318\n",
      "\t\tnum_mapper_calls=312913\n",
      "\t\tnum_reducer_calls=1\n",
      "Streaming final output from complaints...\n",
      "Removing temp directory /tmp/complaints_word_count_combiner.cloudera.20160920.175242.965136...\n",
      "Traceback (most recent call last):\n",
      "  File \"complaints_word_count_combiner.py\", line 42, in <module>\n",
      "    MRComplaintFrequencyCount.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 238, in run_job\n",
      "    self.stdout.flush()\n",
      "IOError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo rm complaints/part*\n",
    "sudo chmod -R 777 complaints/\n",
    "python complaints_word_count_combiner.py Consumer_complaints.csv  --jobconf mapred.reduce.tasks=2 --output-dir complaints | head -10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50\n",
      "\n",
      "\"loan\"\t[119630, 0.11961648333738288]\n",
      "\"modification\"\t[70487, 0.07047903586894681]\n",
      "\"credit\"\t[55251, 0.055244757342420306]\n",
      "\"servicing\"\t[36767, 0.03676284579842478]\n",
      "\"report\"\t[34903, 0.03489905640662605]\n",
      "\"incorrect\"\t[29133, 0.029129708342957247]\n",
      "\"on\"\t[29069, 0.029065715574140123]\n",
      "\"information\"\t[29069, 0.029065715574140123]\n",
      "\"or\"\t[22533, 0.02253045405869137]\n",
      "\"account\"\t[20681, 0.020678663311045852]\n",
      "\"debt\"\t[19309, 0.019306818329528762]\n",
      "\"and\"\t[16448, 0.016446141586000784]\n",
      "\"opening\"\t[16205, 0.016203169041898266]\n",
      "\"health\"\t[12545, 0.012543582575169005]\n",
      "\"club\"\t[12545, 0.012543582575169005]\n",
      "\"not\"\t[12353, 0.012351604268717635]\n",
      "\"owed\"\t[11848, 0.011846661327270018]\n",
      "\"d\"\t[11848, 0.011846661327270018]\n",
      "\"cont\"\t[11848, 0.011846661327270018]\n",
      "\"collect\"\t[11848, 0.011846661327270018]\n",
      "\"attempts\"\t[11848, 0.011846661327270018]\n",
      "\"of\"\t[10885, 0.010883770133974862]\n",
      "\"my\"\t[10731, 0.010729787534008658]\n",
      "\"withdrawals\"\t[10555, 0.010553807419761568]\n",
      "\"deposits\"\t[10555, 0.010553807419761568]\n",
      "\"problems\"\t[9484, 0.009482928429087514]\n",
      "\"application\"\t[8868, 0.008866998029222698]\n",
      "\"to\"\t[8401, 0.008400050794260249]\n",
      "\"unable\"\t[8178, 0.008177075990413084]\n",
      "\"billing\"\t[8158, 0.008157078250157733]\n",
      "\"other\"\t[7886, 0.007885108982684956]\n",
      "\"disputes\"\t[6938, 0.006937216094581312]\n",
      "\"tactics\"\t[6920, 0.0069192181283514965]\n",
      "\"communication\"\t[6920, 0.0069192181283514965]\n",
      "\"reporting\"\t[6559, 0.0065582589167424085]\n",
      "\"lease\"\t[6337, 0.00633628399990801]\n",
      "\"the\"\t[6248, 0.006247294055771698]\n",
      "\"low\"\t[5663, 0.005662360153302677]\n",
      "\"funds\"\t[5663, 0.005662360153302677]\n",
      "\"caused\"\t[5663, 0.005662360153302677]\n",
      "\"by\"\t[5663, 0.005662360153302677]\n",
      "\"being\"\t[5663, 0.005662360153302677]\n",
      "\"process\"\t[5505, 0.005504378005285403]\n",
      "\"verification\"\t[5214, 0.005213410884570044]\n",
      "\"disclosure\"\t[5214, 0.005213410884570044]\n",
      "\"managing\"\t[5006, 0.005005434385914392]\n",
      "\"s\"\t[4858, 0.004857451108024794]\n",
      "\"investigation\"\t[4858, 0.004857451108024794]\n",
      "\"company\"\t[4858, 0.004857451108024794]\n",
      "\"identity\"\t[4729, 0.004728465683377778]\n",
      "\n",
      "Bottom 10\n",
      "\n",
      "\"apply\"\t[118, 0.00011798666750657176]\n",
      "\"amount\"\t[98, 9.798892725122061e-05]\n",
      "\"payment\"\t[92, 9.198960517461527e-05]\n",
      "\"credited\"\t[92, 9.198960517461527e-05]\n",
      "\"convenience\"\t[75, 7.499152595756679e-05]\n",
      "\"checks\"\t[75, 7.499152595756679e-05]\n",
      "\"day\"\t[71, 7.099197790649657e-05]\n",
      "\"amt\"\t[71, 7.099197790649657e-05]\n",
      "\"missing\"\t[64, 6.399276881712366e-05]\n",
      "\"disclosures\"\t[64, 6.399276881712366e-05]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "printf \"Top 50\\n\\n\"\n",
    "cat complaints/part* | sort -k3,3 -gr | head -50\n",
    "printf \"\\nBottom 10\\n\\n\"\n",
    "cat complaints/part* | sort -k3,3 -gr | tail -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-20 13:50:17--  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.4.1\n",
      "Connecting to www.dropbox.com|162.125.4.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/QcwNhjKDBiQDoItffqe7QmQhIaSkS6AkVVLrrUNiUcUhEYNuAy7YYf3feXKlmeDk/file [following]\n",
      "--2016-09-20 13:50:19--  https://dl.dropboxusercontent.com/content_link/QcwNhjKDBiQDoItffqe7QmQhIaSkS6AkVVLrrUNiUcUhEYNuAy7YYf3feXKlmeDk/file\n",
      "Resolving dl.dropboxusercontent.com... 108.160.173.69\n",
      "Connecting to dl.dropboxusercontent.com|108.160.173.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3458517 (3.3M) [text/plain]\n",
      "Saving to: 'ProductPurchaseData.txt?dl=0'\n",
      "\n",
      "ProductPurchaseData 100%[===================>]   3.30M  6.94MB/s    in 0.5s    \n",
      "\n",
      "2016-09-20 13:50:20 (6.94 MB/s) - 'ProductPurchaseData.txt?dl=0' saved [3458517/3458517]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 > ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n",
      "Password:\n"
     ]
    }
   ],
   "source": [
    "!sudo touch shopping_cart.py\n",
    "!sudo chmod 777 shopping_cart.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting shopping_cart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile shopping_cart.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#Same as above but we run on complaints data and isolate the issue column\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRShoppingCart(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        basket = len(WORD_RE.findall(line))\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), (1,basket))\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # sum the words we've seen so far\n",
    "        #I want to sum the words but also hold on to the basket \n",
    "        total=0\n",
    "        for count in counts:\n",
    "            sums, basket = count\n",
    "            total+=sums\n",
    "        yield word, (total, basket)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        self.increment_counter('group','num_reducer_calls',1)  \n",
    "        wordCount = 0\n",
    "        basketSize = 0\n",
    "        for count in counts:\n",
    "            sums, basket = count\n",
    "            basketSize = basket if basket > basketSize else basketSize\n",
    "            wordCount+=sums\n",
    "        \n",
    "        yield None, (word, wordCount, basketSize)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, product_counts):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        prodCounts = defaultdict(int)\n",
    "        maxBasket = 0\n",
    "        total=0\n",
    "        for count in product_counts:\n",
    "            prod, prodCount, basket = count\n",
    "            maxBasket = basket if basket > maxBasket else maxBasket\n",
    "            prodCounts[prod]+=prodCount\n",
    "            total+=prodCount\n",
    "\n",
    "        for k,v in prodCounts.iteritems():\n",
    "            yield k, (v, float(v)/total, maxBasket)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRShoppingCart.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/shopping_cart.cloudera.20160920.195409.792109\n",
      "Counters: 1\n",
      "\tgroup\n",
      "\t\tnum_reducer_calls=12592\n",
      "Running step 2 of 2...\n",
      "Streaming final output from shopping...\n",
      "\"sna56940\"\t[2, 5.251769846438249e-06, 37]\n",
      "\"dai65887\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"gro63887\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"gro15558\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"fro15018\"\t[2, 5.251769846438249e-06, 37]\n",
      "\"sna27421\"\t[9, 2.3632964308972124e-05, 37]\n",
      "\"sna40906\"\t[2, 5.251769846438249e-06, 37]\n",
      "\"sna64909\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"fro65893\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"fro39997\"\t[7, 1.8381194462533874e-05, 37]\n",
      "Removing temp directory /tmp/shopping_cart.cloudera.20160920.195409.792109...\n",
      "Traceback (most recent call last):\n",
      "  File \"shopping_cart.py\", line 68, in <module>\n",
      "    MRShoppingCart.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 237, in run_job\n",
      "    self.stdout.write(line)\n",
      "IOError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod -R 777 shopping/\n",
    "!python shopping_cart.py ProductPurchaseData.txt  --jobconf mapred.reduce.tasks=1 --output-dir shopping | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50\n",
      "\n",
      "\"dai62779\"\t[6667, 0.017506774783101905, 37]\n",
      "\"fro40251\"\t[3881, 0.010191059387013424, 37]\n",
      "\"ele17451\"\t[3875, 0.010175304077474108, 37]\n",
      "\"gro73461\"\t[3602, 0.009458437493435288, 37]\n",
      "\"sna80324\"\t[3044, 0.007993193706279015, 37]\n",
      "\"ele32164\"\t[2851, 0.007486397916097725, 37]\n",
      "\"dai75645\"\t[2736, 0.007184421149927526, 37]\n",
      "\"sna45677\"\t[2455, 0.006446547486502951, 37]\n",
      "\"fro31317\"\t[2330, 0.006118311871100561, 37]\n",
      "\"dai85309\"\t[2293, 0.006021154128941453, 37]\n",
      "\"ele26917\"\t[2292, 0.006018528244018234, 37]\n",
      "\"fro80039\"\t[2233, 0.005863601033548306, 37]\n",
      "\"gro21487\"\t[2115, 0.005553746612608449, 37]\n",
      "\"sna99873\"\t[2083, 0.005469718295065437, 37]\n",
      "\"gro59710\"\t[2004, 0.005262273386131126, 37]\n",
      "\"gro71621\"\t[1920, 0.0050416990525807195, 37]\n",
      "\"fro85978\"\t[1918, 0.005036447282734282, 37]\n",
      "\"gro30386\"\t[1840, 0.00483162825872319, 37]\n",
      "\"ele74009\"\t[1816, 0.004768607020565931, 37]\n",
      "\"gro56726\"\t[1784, 0.0046845787030229185, 37]\n",
      "\"dai63921\"\t[1773, 0.004655693968867509, 37]\n",
      "\"gro46854\"\t[1756, 0.004611053925172783, 37]\n",
      "\"ele66600\"\t[1713, 0.004498140873474361, 37]\n",
      "\"dai83733\"\t[1712, 0.004495514988551142, 37]\n",
      "\"fro32293\"\t[1702, 0.004469256139318951, 37]\n",
      "\"ele66810\"\t[1697, 0.0044561267147028545, 37]\n",
      "\"sna55762\"\t[1646, 0.00432220658361868, 37]\n",
      "\"dai22177\"\t[1627, 0.004272314770077516, 37]\n",
      "\"fro78087\"\t[1531, 0.00402022981744848, 37]\n",
      "\"ele99737\"\t[1516, 0.003980841543600193, 37]\n",
      "\"gro94758\"\t[1489, 0.003909942650673277, 37]\n",
      "\"ele34057\"\t[1489, 0.003909942650673277, 37]\n",
      "\"fro35904\"\t[1436, 0.0037707707497426635, 37]\n",
      "\"fro53271\"\t[1420, 0.003728756590971157, 37]\n",
      "\"sna93860\"\t[1407, 0.0036946200869693085, 37]\n",
      "\"sna90094\"\t[1390, 0.0036499800432745837, 37]\n",
      "\"gro38814\"\t[1352, 0.0035501964161922567, 37]\n",
      "\"ele56788\"\t[1345, 0.003531815221729723, 37]\n",
      "\"gro61133\"\t[1321, 0.003468793983572464, 37]\n",
      "\"ele74482\"\t[1316, 0.0034556645589563684, 37]\n",
      "\"dai88807\"\t[1316, 0.0034556645589563684, 37]\n",
      "\"ele59935\"\t[1311, 0.003442535134340273, 37]\n",
      "\"sna96271\"\t[1295, 0.0034005209755687666, 37]\n",
      "\"dai43223\"\t[1290, 0.003387391550952671, 37]\n",
      "\"ele91337\"\t[1289, 0.0033847656660294517, 37]\n",
      "\"gro15017\"\t[1275, 0.003348003277104384, 37]\n",
      "\"dai31081\"\t[1261, 0.0033112408881793166, 37]\n",
      "\"gro81087\"\t[1220, 0.0032035796063273323, 37]\n",
      "\"dai22896\"\t[1219, 0.0032009537214041134, 37]\n",
      "\"gro85051\"\t[1214, 0.0031878242967880175, 37]\n",
      "\n",
      "Bottom 10\n",
      "\n",
      "\"dai12448\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai12152\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai12139\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11995\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11946\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11707\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11582\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11375\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11273\"\t[1, 2.6258849232191246e-06, 37]\n",
      "\"dai11257\"\t[1, 2.6258849232191246e-06, 37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "printf \"Top 50\\n\\n\"\n",
    "cat shopping//part* | sort -k3,3 -gr | head -50\n",
    "printf \"\\nBottom 10\\n\\n\"\n",
    "cat shopping//part* | sort -k3,3 -gr | tail -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "sudo touch shopping_pairs.py\n",
    "sudo chmod 777 shopping_pairs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting shopping_pairs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile shopping_pairs.py\n",
    "\n",
    "#Pair, # occurrences, relative # occurrences \n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRShoppingPairs(MRJob):\n",
    "    \n",
    "    def map_basket_pairs(self, _, basket):\n",
    "        #Get all combinations of pairs, turn to set, then iterate and spit out \n",
    "        combos = itertools.combinations(sorted(WORD_RE.findall(basket)), 2)\n",
    "        for combo in combos:\n",
    "            yield combo, 1\n",
    "        yield \"basket\", 1\n",
    "    \n",
    "    def group_basket_pairs(self, key, values):\n",
    "        total = 0 \n",
    "        for value in values:\n",
    "            total += value\n",
    "        if total>=100:\n",
    "            yield None, (key, total)\n",
    "    \n",
    "    def final_basket_sum(self, _, values):\n",
    "        values, valuesCp = itertools.tee(values)\n",
    "        #[total for value in values for key, total in value if key==\"basket\"]\n",
    "        for value in valuesCp:\n",
    "            key, total = value\n",
    "            if key == \"basket\":\n",
    "                basketSize = total\n",
    "        \n",
    "        for value in values:\n",
    "            key, total = value\n",
    "            yield key, (total, float(total)/basketSize)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.map_basket_pairs, reducer=self.group_basket_pairs),\n",
    "                MRStep(reducer=self.final_basket_sum)]\n",
    "if __name__=='__main__':\n",
    "    MRShoppingPairs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"SNA59903\", \"SNA72163\"]\t[310, 0.009967525159962702]\n",
      "[\"SNA72163\", \"SNA80324\"]\t[116, 0.003729783608244108]\n",
      "[\"SNA72163\", \"SNA93860\"]\t[121, 0.003890550143082216]\n",
      "[\"SNA74022\", \"SNA96271\"]\t[107, 0.0034404038455355134]\n",
      "[\"SNA80324\", \"SNA90094\"]\t[154, 0.00495160927301373]\n",
      "[\"SNA80324\", \"SNA93860\"]\t[150, 0.004822996045143243]\n",
      "[\"SNA80324\", \"SNA96271\"]\t[219, 0.007041574225909134]\n",
      "[\"SNA80324\", \"SNA99873\"]\t[163, 0.005240989035722324]\n",
      "[\"SNA90094\", \"SNA96271\"]\t[104, 0.0033439439246326485]\n",
      "[\"SNA93860\", \"SNA99873\"]\t[105, 0.00337609723160027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/shopping_pairs.cloudera.20160921.012658.327054\n",
      "Running step 2 of 2...\n",
      "Streaming final output from shopping_pairs...\n",
      "Removing temp directory /tmp/shopping_pairs.cloudera.20160921.012658.327054...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo chmod -R 777 shopping_pairs/\n",
    "python shopping_pairs.py ProductPurchaseData.txt  --jobconf mapred.reduce.tasks=4 --output-dir shopping_pairs | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50\n",
      "\n",
      "[\"DAI62779\", \"ELE17451\"]\t[1592, 0.05118806469245362]\n",
      "[\"FRO40251\", \"SNA80324\"]\t[1412, 0.04540046943828173]\n",
      "[\"DAI75645\", \"FRO40251\"]\t[1254, 0.04032024693739751]\n",
      "[\"FRO40251\", \"GRO85051\"]\t[1213, 0.039001961351725026]\n",
      "[\"DAI62779\", \"GRO73461\"]\t[1139, 0.03662261663612103]\n",
      "[\"DAI75645\", \"SNA80324\"]\t[1130, 0.03633323687341243]\n",
      "[\"DAI62779\", \"FRO40251\"]\t[1070, 0.03440403845535513]\n",
      "[\"DAI62779\", \"SNA80324\"]\t[923, 0.029677502331114755]\n",
      "[\"DAI62779\", \"DAI85309\"]\t[918, 0.029516735796276648]\n",
      "[\"ELE32164\", \"GRO59710\"]\t[911, 0.029291662647503297]\n",
      "[\"FRO40251\", \"GRO73461\"]\t[882, 0.02835921674544227]\n",
      "[\"DAI62779\", \"DAI75645\"]\t[882, 0.02835921674544227]\n",
      "[\"DAI62779\", \"ELE92920\"]\t[877, 0.02819845021060416]\n",
      "[\"FRO40251\", \"FRO92469\"]\t[835, 0.026848011317964052]\n",
      "[\"DAI62779\", \"ELE32164\"]\t[832, 0.026751551397061188]\n",
      "[\"DAI75645\", \"GRO73461\"]\t[712, 0.022893154560946594]\n",
      "[\"DAI43223\", \"ELE32164\"]\t[711, 0.022861001253978972]\n",
      "[\"DAI62779\", \"GRO30386\"]\t[709, 0.02279669464004373]\n",
      "[\"ELE17451\", \"FRO40251\"]\t[697, 0.022410854956432268]\n",
      "[\"DAI85309\", \"ELE99737\"]\t[659, 0.021189029291662647]\n",
      "[\"DAI62779\", \"ELE26917\"]\t[650, 0.020899649528954053]\n",
      "[\"GRO21487\", \"GRO73461\"]\t[631, 0.02028873669656924]\n",
      "[\"DAI62779\", \"SNA45677\"]\t[604, 0.019420597408443457]\n",
      "[\"ELE17451\", \"SNA80324\"]\t[597, 0.019195524259670107]\n",
      "[\"DAI62779\", \"GRO71621\"]\t[595, 0.019131217645734864]\n",
      "[\"DAI62779\", \"SNA55762\"]\t[593, 0.01906691103179962]\n",
      "[\"DAI62779\", \"DAI83733\"]\t[586, 0.01884183788302627]\n",
      "[\"ELE17451\", \"GRO73461\"]\t[580, 0.018648918041220538]\n",
      "[\"GRO73461\", \"SNA80324\"]\t[562, 0.01807015851580335]\n",
      "[\"DAI62779\", \"GRO59710\"]\t[561, 0.01803800520883573]\n",
      "[\"DAI62779\", \"FRO80039\"]\t[550, 0.01768431883219189]\n",
      "[\"DAI75645\", \"ELE17451\"]\t[547, 0.017587858911289025]\n",
      "[\"DAI62779\", \"SNA93860\"]\t[537, 0.01726632584161281]\n",
      "[\"DAI55148\", \"DAI62779\"]\t[526, 0.016912639464968973]\n",
      "[\"DAI43223\", \"GRO59710\"]\t[512, 0.01646249316742227]\n",
      "[\"ELE17451\", \"ELE32164\"]\t[511, 0.016430339860454647]\n",
      "[\"DAI62779\", \"SNA18336\"]\t[506, 0.01626957332561654]\n",
      "[\"ELE32164\", \"GRO73461\"]\t[486, 0.015626507186264106]\n",
      "[\"DAI85309\", \"ELE17451\"]\t[482, 0.01549789395839362]\n",
      "[\"DAI62779\", \"FRO78087\"]\t[482, 0.01549789395839362]\n",
      "[\"DAI62779\", \"GRO94758\"]\t[479, 0.015401434037490756]\n",
      "[\"GRO85051\", \"SNA80324\"]\t[471, 0.015144207581749784]\n",
      "[\"DAI62779\", \"GRO21487\"]\t[471, 0.015144207581749784]\n",
      "[\"ELE17451\", \"GRO30386\"]\t[468, 0.015047747660846917]\n",
      "[\"FRO85978\", \"SNA95666\"]\t[463, 0.01488698112600881]\n",
      "[\"DAI62779\", \"FRO19221\"]\t[462, 0.014854827819041188]\n",
      "[\"DAI62779\", \"GRO46854\"]\t[461, 0.014822674512073567]\n",
      "[\"DAI43223\", \"DAI62779\"]\t[459, 0.014758367898138324]\n",
      "[\"ELE92920\", \"SNA18336\"]\t[455, 0.014629754670267838]\n",
      "[\"DAI88079\", \"FRO40251\"]\t[446, 0.014340374907559243]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "printf \"Top 50\\n\\n\"\n",
    "cat shopping_pairs/part* | sort -k4,4 -gr | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OPTIONAL: all HW below this are optional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)\n",
    "\n",
    "Dataset description\n",
    "For this assignment you will explore a set of 100,000 Wikipedia documents:\n",
    "\n",
    "https://www.dropbox.com/s/n5lfbnztclo93ej/wikitext_100k.txt?dl=0\n",
    "s3://cs9223/wikitext_100k.txt, or\n",
    "https://s3.amazonaws.com/cs9223/wikitext_100k.txt\n",
    "Each line in this file consists of the plain text extracted from a Wikipedia document.\n",
    "\n",
    "Task\n",
    "Compute the relative frequencies of each word that occurs in the documents in wikitext_100k.txt and output the top 100 word pairs sorted by decreasing order of relative frequency.\n",
    "\n",
    "Recall that the relative frequency (RF) of word B given word A is defined as follows:\n",
    "\n",
    "   f(B|A) = Count(A, B) / Count (A)   =  Count(A, B) / sum_B'(Count (A, B')\n",
    "\n",
    "where count(A,B) is the number of times A and B co-occur within a window of two words (co-occurrence window size of two) in a document and count(A) the number of times A occurs with anything else. Intuitively, given a document collection, the relative frequency captures the proportion of time the word B appears in the same document as A. (See Section 3.3, in Data-Intensive Text Processing with MapReduce).\n",
    "\n",
    "In the async lecture you learned different approaches to do this, and in this assignment, you will implement them:\n",
    "\n",
    "a.\tWrite a mapreduce program which uses the Stripes approach and writes its output in a file named rfstripes.txt \n",
    "\n",
    "b.\tWrite a mapreduce program which uses the Pairs approach and writes its output in a file named rfpairs.txt\n",
    "\n",
    "c.\tCompare the performance of the two approaches and output the relative performance to a file named rfcomp.txt. Compute the relative performance as follows: (running time for Pairs/ running time for Stripes). Also include an analysis comparing the communication costs for the two approaches. Instrument your mapper and reduces for counters where necessary to aid with your analysis.\n",
    "\n",
    "NOTE: please limit your analysis to the top 100 word pairs sorted by decreasing order of relative frequency for each word (tokens with all alphabetical letters).\n",
    "\n",
    "Please include markdown cell named rf.txt that describes the following:\n",
    "\n",
    "the input/output format in each Hadoop task, i.e., the keys for the mappers and reducers\n",
    "the Hadoop cluster settings you used, i.e., number of mappers and reducers\n",
    "the running time for each approach: pairs and stripes\n",
    "\n",
    "You can write your program using Python or MrJob (with Hadoop streaming) and you should run it on AWS. It is a good idea to develop and test your program on a local machine  before deploying on AWS. Remember your notebook, needs to have all the commands you used to run each Mapreduce job (i.e., pairs and stripes) -- include the Hadoop streaming commands you used to run your jobs.\n",
    "\n",
    "In addition the All the following files should be compressed in one ZIP file and submitted. The ZIP file should contain:\n",
    "\n",
    "\n",
    "A.\tThe result files: rfstripes.txt, rfpairs.txt, rfcomp.txt\n",
    "\n",
    "Prior to working with Hadoop, the corpus should first be preprocessed as follows:\n",
    "perform tokenization (whitespace and all non-alphabetic characters) and stopword removal  using standard tools from the Lucene search engine. All tokens should  then be replaced\n",
    "with unique integers for a more efficient encoding. \n",
    "\n",
    "\n",
    "== Preliminary information for the remaing HW problems===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.7 Apriori Algorithm\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.8. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HW3.8\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "END OF HOMEWORK\n",
    "==============="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
