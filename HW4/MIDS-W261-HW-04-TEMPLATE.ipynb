{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====DATSCIW261 ASSIGNMENT #4=====\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #4\n",
    "\n",
    "Version 2016-05-27 (FINAL)\n",
    "\n",
    "# === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Submit your homework via the following form by  8:00AM of the following Tuesday (West Coast Time):\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "# === Week 4 ASSIGNMENTS ===\n",
    "\n",
    "# HW 4.0. \n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?\n",
    "\n",
    "# HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "When it used in these frameworks? \n",
    "What is the default serialization mode for input and outputs for MrJob? \n",
    "\n",
    "\n",
    "# HW 4.2: Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "- C,\"10001\",10001   #Visitor id 10001\n",
    "- V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "- V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "- V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "- C,\"10002\",10002   #Visitor id 10001\n",
    "- V\n",
    "- Note: #denotes comments\n",
    "- to the format:\n",
    "\n",
    "- V,1000,1,C, 10001\n",
    "- V,1001,1,C, 10001\n",
    "- V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this.\n",
    "\n",
    "# HW 4.3: Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).\n",
    "\n",
    "# HW 4.4: Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.\n",
    "\n",
    "# HW 4.5 Clustering Tweet Dataset\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the \n",
    "(row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "Row 1: Words\n",
    "Row 2: Aggregated distribution across all classes\n",
    "Row 3-6 class-aggregated distributions for clases 0-3\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "\n",
    "(2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "##Geneate random initial centroids around the global aggregate\n",
    "##Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    #perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "——\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "\n",
    "\n",
    "# HW4.6  (OPTIONAL) Scaleable K-MEANS++ \n",
    "\n",
    "Over half a century old and showing no signs of aging,\n",
    "k-means remains one of the most popular data processing\n",
    "algorithms. As is well-known, a proper initialization\n",
    "of k-means is crucial for obtaining a good final solution.\n",
    "The recently proposed k-means++ initialization algorithm\n",
    "achieves this, obtaining an initial set of centers that is provably\n",
    "close to the optimum solution. A major downside of the\n",
    "k-means++ is its inherent sequential nature, which limits its\n",
    "applicability to massive data: one must make k passes over\n",
    "the data to find a good initial set of centers. The paper listed below \n",
    "shows how to drastically reduce the number of passes needed\n",
    "to obtain, in parallel, a good initialization. This is unlike\n",
    "prevailing efforts on parallelizing k-means that have mostly\n",
    "focused on the post-initialization phases of k-means. The \n",
    "proposed initialization algorithm k-means||\n",
    "obtains a nearly optimal solution after a logarithmic number\n",
    "of passes; the paper also shows that in practice a constant\n",
    "number of passes suffices. Experimental evaluation on realworld\n",
    "large-scale data demonstrates that k-means|| outperforms\n",
    "k-means++ in both sequential and parallel settings.\n",
    "\n",
    "Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf \n",
    "\n",
    "In MrJob, implement K-MEANS|| and compare with a random initializtion when used in \n",
    "conjunction with the kmeans algorithm as an initialization step for the 2D  dataset \n",
    "generated using code in the following notebook:\n",
    "\n",
    "https://www.dropbox.com/s/lbzwmyv0d8rocfq/MrJobKmeans.ipynb?dl=0\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the K-MEANS|| algorithms iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "\n",
    "## 4.6.1 Apply your implementation of K-MEANS|| to the dataset  in HW 4.5 and compare to the a \n",
    "random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HW4.7   (OPTIONAL) Canopy Clustering\n",
    "\n",
    "An alternative way to intialize the k-means algorithm is the  canopy clustering. The canopy clustering \n",
    "algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and \n",
    "Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the \n",
    "Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, \n",
    "where using another algorithm directly may be impractical due to the size of the data set.\n",
    "\n",
    "For more details on the Canopy Clustering algorithm see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Canopy_clustering_algorithm\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the Canopy Clustering based K-MEANS algorithm iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "## 4.7.1 Apply your implementation Canopy Clustering based K-MEANS algorithm to the dataset  in HW 4.5 and compare to the a \n",
    "random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "\n",
    "\n",
    "# =====================\n",
    "# END OF HOMEWORK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 4.2: Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "- C,\"10001\",10001   #Visitor id 10001\n",
    "- V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "- V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "- V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "- C,\"10002\",10002   #Visitor id 10001\n",
    "- V\n",
    "- Note: #denotes comments\n",
    "- to the format:\n",
    "\n",
    "- V,1000,1,C, 10001\n",
    "- V,1001,1,C, 10001\n",
    "- V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: Data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\r",
      "  1 48.5M    1  623k    0     0  97044      0  0:08:44  0:00:06  0:08:38  793k\r",
      " 15 48.5M   15 7727k    0     0  1019k      0  0:00:48  0:00:07  0:00:41 4326k\r",
      " 28 48.5M   28 13.6M    0     0  1624k      0  0:00:30  0:00:08  0:00:22 5001k\r",
      " 40 48.5M   40 19.8M    0     0  2123k      0  0:00:23  0:00:09  0:00:14 5372k\r",
      " 53 48.5M   53 26.0M    0     0  2523k      0  0:00:19  0:00:10  0:00:09 5577k\r",
      " 64 48.5M   64 31.2M    0     0  2760k      0  0:00:18  0:00:11  0:00:07 6265k\r",
      " 71 48.5M   71 34.7M    0     0  2826k      0  0:00:17  0:00:12  0:00:05 5564k\r",
      " 80 48.5M   80 39.1M    0     0  2952k      0  0:00:16  0:00:13  0:00:03 5226k\r",
      " 88 48.5M   88 43.0M    0     0  3020k      0  0:00:16  0:00:14  0:00:02 4736k\r",
      " 96 48.5M   96 47.0M    0     0  3094k      0  0:00:16  0:00:15  0:00:01 4302k\r",
      "100 48.5M  100 48.5M    0     0  3121k      0  0:00:15  0:00:15 --:--:-- 4083k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r",
      " 80 3377k   80 2735k    0     0  1040k      0  0:00:03  0:00:02  0:00:01 2966k\r",
      "100 3377k  100 3377k    0     0  1241k      0  0:00:02  0:00:02 --:--:-- 3334k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100 2493k  100 2493k    0     0  1330k      0  0:00:01  0:00:01 --:--:-- 2655k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r",
      "100 31952  100 31952    0     0  28669      0  0:00:01  0:00:01 --:--:-- 1200k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  297k  100  297k    0     0   234k      0  0:00:01  0:00:01 --:--:--  234k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir Data\n",
    "curl -L https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 -o Data/Consumer_Complaints.csv\n",
    "curl -L https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 -o Data/ProductPurchaseData.txt\n",
    "curl -L https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt?dl=0 -o Data/topUsers_Apr-Jul_2014_1000-words.txt\n",
    "curl -L https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0 -o Data/topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "curl -L https://kdd.ics.uci.edu/databases/msweb/anonymous-msweb.data.gz -o Data/anonymous-msweb.data.gz\n",
    "gunzip Data/anonymous-msweb.data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mData\u001b[m\u001b[m/                           MIDS-W261-HW-04-TEMPLATE.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "open(\"anonymous-msweb-preprocessed.data\", \"w\").close\n",
    "custID = \"NA\"\n",
    "with open(\"Data/anonymous-msweb.data\", \"r\") as IF:\n",
    "    for line in IF:\n",
    "        line = line.strip()\n",
    "        data = re.split(\",\",line)\n",
    "        if data[0] == \"C\":\n",
    "            custID = data[1]\n",
    "            custID = re.sub(\"\\\"\",\"\",custID)\n",
    "        if data[0] == \"V\" and not custID == \"NA\":\n",
    "            with open(\"anonymous-msweb-preprocessed.data\", \"a\") as OF:\n",
    "                OF.writelines(line+\",\"+\"C\"+\",\"+custID+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top5pages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top5pages.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "import re    \n",
    " \n",
    "class top5pages(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    pages = [\"NA\",\"NA\",\"NA\",\"NA\",\"NA\"]\n",
    "    counts = [0,0,0,0,0]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper = self.mapper,\n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer,\n",
    "                reducer_final = self.reducer_final\n",
    "                )]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        data = re.split(\",\",line)\n",
    "        pageID = data[1]\n",
    "        yield pageID,1\n",
    "        \n",
    "    def combiner(self,pageID,counts):\n",
    "        count = sum(counts)\n",
    "        yield pageID,count\n",
    "\n",
    "    def reducer(self,pageID,counts):\n",
    "        count = sum(counts)\n",
    "        ix = -1\n",
    "        for i in range(5):\n",
    "            if count > self.counts[i]:\n",
    "                ix = i\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if ix >= 0:\n",
    "            self.counts.insert(ix+1,count)\n",
    "            self.pages.insert(ix+1,pageID)\n",
    "            self.counts = self.counts[1:6]\n",
    "            self.pages = self.pages[1:6]\n",
    "\n",
    "    def reducer_final(self):\n",
    "        self.counts.reverse()\n",
    "        self.pages.reverse()\n",
    "        print \"The top 5 pages are:\"\n",
    "        for i in range(5):\n",
    "            yield None,self.pages[i] + \",\" + str(self.counts[i])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    top5pages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x top5pages.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/top5pages.cloudera.20160930.045315.554220\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/top5pages.cloudera.20160930.045315.554220/output...\n",
      "Removing temp directory /tmp/top5pages.cloudera.20160930.045315.554220...\n"
     ]
    }
   ],
   "source": [
    "!./top5pages.py anonymous-msweb-preprocessed.data > top5pages.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
