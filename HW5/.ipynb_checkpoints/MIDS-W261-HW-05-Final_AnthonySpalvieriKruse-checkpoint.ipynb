{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Anthony Spalvieri-Kruse   \n",
    "__Class:__ MIDS w261 Fall 2016 Group 1  \n",
    "__Email:__  ask@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW5.0](#1.0)   \n",
    "    1.0.  [HW5.1](#1.1)   \n",
    "    1.2.  [HW5.2](#1.2)   \n",
    "    1.3.  [HW5.3](#1.3)    \n",
    "    1.4.  [HW5.4](#1.4)    \n",
    "    1.5.  [HW5.5](#1.5)    \n",
    "    1.5.  [HW5.6](#1.6)    \n",
    "    1.5.  [HW5.7](#1.7)    \n",
    "    1.5.  [HW5.8](#1.8)    \n",
    "    1.5.  [HW5.9](#1.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2016-09-25 \n",
    "\n",
    " === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "HW4 can be completed locally on your computer\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week\n",
    "\n",
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.0  <a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A data warehouse is a central repository for data from various sources, structured specifically for analytics as opposed to transactions like in a standard online transactional processing database.  A star schema splits a set of data into facts and dimensions, where facts are the measurable, quantitative data, and dimensions are generally expressed as lookup tables that provided descriptive attributes related to a fact.  Star schema's are typically used for data warehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "        3NF stands for third normal form, which is a subset of 1st and 2nd normal form.  It's characteristics are as follows: 1) no column contains multiple entries in a cell, 2) No columns are dependent on a non-primary key, 3) Non-key columns are dependent on the entire key.  Machine learning at scale would generally benefit from denormalized data rather than data  in 3NF, particularly because 3NF data requires look-ups to various dimension attributes, and this could prove costly/prohibitive in a distributed framework.\n",
    "\n",
    "- In what form does ML consume data?\n",
    "\n",
    "        Generally ML uses data in the form of (label, features), which would be best expressed through denormalized data because it incorporates all fields into a single data strip, even if those fields are redundant according to 2nd and 3rd normal form constraints.\n",
    "\n",
    "\n",
    "- Why would one use log files that are denormalized?\n",
    "    \n",
    "        When we denormalize data we're adding redundant information back into a line of data, and this could be very useful in a parallel computation framework because it removes dependencies.  While we may be able to obtain a look-up dimension by using a foreign key in a line of data, this would be bad in hadoop because the value component of the key, value pair would not be sufficient to perform the computation without doing a lookup across the network (or by keeping the extra data in memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.2  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hashside_joins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hashside_joins.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "class HashsideJoin(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(HashsideJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\"--join_type\", type=\"str\")\n",
    "        self.add_passthrough_option(\"--right_table_length\", type=\"int\")\n",
    "        self.add_file_option(\"--left_table\")\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(HashsideJoin, self).__init__(*args, **kwargs)\n",
    "        self.join_type = self.options.join_type\n",
    "        self.right_table_length = self.options.right_table_length\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.urlTable = {}\n",
    "        self.keyMatch = {}\n",
    "        with open(self.options.left_table, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip(\"\\n\").split(\",\")\n",
    "                pageId = line[1]\n",
    "                leftTableRow = line[:1] + line[2:]\n",
    "                self.urlTable[pageId] = leftTableRow\n",
    "                self.keyMatch[pageId] = False\n",
    "\n",
    "    #Emit Only matches\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        pageId = line[1]\n",
    "        rightTableRow = line[:1]+line[2:]\n",
    "        \n",
    "        if self.join_type == \"inner\":\n",
    "            if pageId in self.urlTable.keys():\n",
    "                value = self.urlTable[pageId] + rightTableRow\n",
    "                value = \",\".join(value)\n",
    "                yield pageId,value\n",
    "        if self.join_type == \"right\":\n",
    "            #Need to output the rightTableRow no matter what, \n",
    "            #i'm either padding with Nulls, or i'm tacking on the key match\n",
    "            if pageId in self.urlTable.keys():\n",
    "                value = self.urlTable[pageId] + rightTableRow\n",
    "                value = \",\".join(value)\n",
    "            else:\n",
    "                value = [\"null\"]*len(self.urlTable.values()[0]) + rightTableRow\n",
    "                value = \",\".join(value)\n",
    "            yield pageId, value\n",
    "        if self.join_type == \"left\":\n",
    "            if pageId in self.urlTable.keys():\n",
    "                value = self.urlTable[pageId] + rightTableRow\n",
    "                value = \",\".join(value)\n",
    "                self.keyMatch[pageId] = True\n",
    "                yield pageId,value    \n",
    "                \n",
    "    def mapper_final(self):\n",
    "        if self.join_type == \"left\":\n",
    "            for key in self.keyMatch.keys():\n",
    "                #If there were right table keys matching the left table key \n",
    "                if self.keyMatch[key] == False:\n",
    "                    #Output Null padded rows \n",
    "                    value = self.urlTable[key] + [\"null\"]*self.right_table_length\n",
    "                    value = \",\".join(value)\n",
    "                    yield key, value\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init, mapper=self.mapper, mapper_final=self.mapper_final)]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    HashsideJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/hashside_joins.ask.20161004.000655.879208\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000655.879208/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob3463049357448527635.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0323\n",
      "  Submitted application application_1473978660783_0323\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0323/\n",
      "  Running job: job_1473978660783_0323\n",
      "  Job job_1473978660783_0323 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1473978660783_0323 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000655.879208/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1756063\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5868333\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=259124\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1756441\n",
      "\t\tHDFS: Number of bytes written=5868333\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=22821888\n",
      "\t\tTotal time spent by all map tasks (ms)=14858\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44574\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14858\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4270\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=59\n",
      "\t\tInput split bytes=378\n",
      "\t\tMap input records=98654\n",
      "\t\tMap output records=98654\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=439418880\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=1632108544\n",
      "\t\tVirtual memory (bytes) snapshot=4382457856\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000655.879208/output...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000655.879208...\n",
      "Removing temp directory /tmp/hashside_joins.ask.20161004.000655.879208...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/hashside_joins.ask.20161004.000750.987634\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000750.987634/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob776833075441555584.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0324\n",
      "  Submitted application application_1473978660783_0324\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0324/\n",
      "  Running job: job_1473978660783_0324\n",
      "  Job job_1473978660783_0324 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1473978660783_0324 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000750.987634/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1756063\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5868333\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=259078\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1756441\n",
      "\t\tHDFS: Number of bytes written=5868333\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=22917120\n",
      "\t\tTotal time spent by all map tasks (ms)=14920\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44760\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14920\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3750\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tInput split bytes=378\n",
      "\t\tMap input records=98654\n",
      "\t\tMap output records=98654\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=511139840\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=3135242240\n",
      "\t\tVirtual memory (bytes) snapshot=4389404672\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000750.987634/output...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000750.987634...\n",
      "Removing temp directory /tmp/hashside_joins.ask.20161004.000750.987634...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/hashside_joins.ask.20161004.000845.768116\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000845.768116/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob8611763683771998273.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0325\n",
      "  Submitted application application_1473978660783_0325\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0325/\n",
      "  Running job: job_1473978660783_0325\n",
      "  Job job_1473978660783_0325 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1473978660783_0325 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000845.768116/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1756063\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5871635\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=259122\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1756441\n",
      "\t\tHDFS: Number of bytes written=5871635\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23510016\n",
      "\t\tTotal time spent by all map tasks (ms)=15306\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=45918\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15306\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4020\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=43\n",
      "\t\tInput split bytes=378\n",
      "\t\tMap input records=98654\n",
      "\t\tMap output records=98704\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=497541120\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=1693450240\n",
      "\t\tVirtual memory (bytes) snapshot=4400062464\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000845.768116/output...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/hashside_joins.ask.20161004.000845.768116...\n",
      "Removing temp directory /tmp/hashside_joins.ask.20161004.000845.768116...\n"
     ]
    }
   ],
   "source": [
    "!./hashside_joins.py anonymous-msweb-preprocessed.data -r hadoop --right_table_length 4 --join_type \"inner\" --left_table JustUrls.txt > inner.txt\n",
    "!./hashside_joins.py anonymous-msweb-preprocessed.data -r hadoop --right_table_length 4 --join_type \"right\" --left_table JustUrls.txt > right.txt\n",
    "!./hashside_joins.py anonymous-msweb-preprocessed.data -r hadoop --right_table_length 4 --join_type \"left\" --left_table JustUrls.txt > left.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98654 inner.txt\n",
      "98654 right.txt\n",
      "98704 left.txt\n",
      "\n",
      "\"1123\"\t\"A,1,\\\"Germany\\\",\\\"/germany\\\",V,1,C,42708\"\n",
      "\"1038\"\t\"A,1,\\\"SiteBuilder Network Membership\\\",\\\"/sbnmember\\\",V,1,C,42708\"\n",
      "\"1026\"\t\"A,1,\\\"Internet Site Construction for Developers\\\",\\\"/sitebuilder\\\",V,1,C,42708\"\n",
      "\"1041\"\t\"A,1,\\\"Developer Workshop\\\",\\\"/workshop\\\",V,1,C,42708\"\n",
      "\"1001\"\t\"A,1,\\\"Support Desktop\\\",\\\"/support\\\",V,1,C,42709\"\n",
      "\"1003\"\t\"A,1,\\\"Knowledge Base\\\",\\\"/kb\\\",V,1,C,42709\"\n",
      "\"1035\"\t\"A,1,\\\"Windows95 Support\\\",\\\"/windowssupport\\\",V,1,C,42710\"\n",
      "\"1001\"\t\"A,1,\\\"Support Desktop\\\",\\\"/support\\\",V,1,C,42710\"\n",
      "\"1018\"\t\"A,1,\\\"isapi\\\",\\\"/isapi\\\",V,1,C,42710\"\n",
      "\"1008\"\t\"A,1,\\\"Free Downloads\\\",\\\"/msdownload\\\",V,1,C,42711\"\n",
      "\n",
      "\"1123\"\t\"A,1,\\\"Germany\\\",\\\"/germany\\\",V,1,C,42708\"\n",
      "\"1038\"\t\"A,1,\\\"SiteBuilder Network Membership\\\",\\\"/sbnmember\\\",V,1,C,42708\"\n",
      "\"1026\"\t\"A,1,\\\"Internet Site Construction for Developers\\\",\\\"/sitebuilder\\\",V,1,C,42708\"\n",
      "\"1041\"\t\"A,1,\\\"Developer Workshop\\\",\\\"/workshop\\\",V,1,C,42708\"\n",
      "\"1001\"\t\"A,1,\\\"Support Desktop\\\",\\\"/support\\\",V,1,C,42709\"\n",
      "\"1003\"\t\"A,1,\\\"Knowledge Base\\\",\\\"/kb\\\",V,1,C,42709\"\n",
      "\"1035\"\t\"A,1,\\\"Windows95 Support\\\",\\\"/windowssupport\\\",V,1,C,42710\"\n",
      "\"1001\"\t\"A,1,\\\"Support Desktop\\\",\\\"/support\\\",V,1,C,42710\"\n",
      "\"1018\"\t\"A,1,\\\"isapi\\\",\\\"/isapi\\\",V,1,C,42710\"\n",
      "\"1008\"\t\"A,1,\\\"Free Downloads\\\",\\\"/msdownload\\\",V,1,C,42711\"\n",
      "\n",
      "\"1199\"\t\"A,1,\\\"feedback\\\",\\\"/feedback\\\",null,null,null,null\"\n",
      "\"1196\"\t\"A,1,\\\"ie40\\\",\\\"/ie40\\\",null,null,null,null\"\n",
      "\"1290\"\t\"A,1,\\\"Activate the Internet Conference\\\",\\\"/devmovies\\\",null,null,null,null\"\n",
      "\"1291\"\t\"A,1,\\\"news\\\",\\\"/news\\\",null,null,null,null\"\n",
      "\"1297\"\t\"A,1,\\\"Central America\\\",\\\"/centroam\\\",null,null,null,null\"\n",
      "\"1294\"\t\"A,1,\\\"Bookshelf\\\",\\\"/bookshelf\\\",null,null,null,null\"\n",
      "\"1248\"\t\"A,1,\\\"Softimage \\\",\\\"/softimage\\\",null,null,null,null\"\n",
      "\"1287\"\t\"A,1,\\\"International AutoRoute\\\",\\\"/autoroute\\\",null,null,null,null\"\n",
      "\"1289\"\t\"A,1,\\\"Master Chef Product Information\\\",\\\"/masterchef\\\",null,null,null,null\"\n",
      "\"1288\"\t\"A,1,\\\"library\\\",\\\"/library\\\",null,null,null,null\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wc -l inner.txt\n",
    "wc -l right.txt\n",
    "wc -l left.txt\n",
    "printf \"\\n\"\n",
    "tail -10 inner.txt\n",
    "printf \"\\n\"\n",
    "tail -10 right.txt\n",
    "printf \"\\n\"\n",
    "tail -10 left.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise I chose the URL only table as my left table, because it was the smaller of the two and thus the easiest one to store into memory.  The inner and right joins have the same number of rows, which makes sense because the set of keys in the customer visit table is a subset of the keys in the url table.  This is also why the left join had the greatest number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3 <a name=\"1.3\"></a> Systems tests on n-grams dataset (Phase1) and full experiment (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "## 3.  HW5.3.0 Run Systems tests locally (PHASE1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. We should calculate the stripes cooccurrence data from the raw text and not from the 5-gram preprocessed data. Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Childs Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _HW 5.4-5.5_,  unit test and regression test your code using the  followings small test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see above]\n",
    "* stripe-docs-test [see below]\n",
    "* atlas-boon-test [see below]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test\n",
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Stripes for systems test 1 (predefined)\n",
    "############################################\n",
    "\n",
    "with open(\"mini_stripes.txt\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat mini_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Phase 1\n",
    "Complete 5.4 and 5.5 and systems test them using the above test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the following results (for stripes, inverted index, and pairwise similarities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "#from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "#Goal: Take in n-gram file and output file w/ structure {Word1: {CoWord1: count1, CoWord2: count2, etc}, etc}\n",
    "class BuildStripes(MRJob):\n",
    "    \n",
    "    def combine_dicts(a, b):\n",
    "        return dict(a.items() + b.items() +\n",
    "            [(k, a[k] + b[k]) for k in set(b) & set(a)])\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        ngram, count, page, book = line.strip(\"\\n\").split(\"\\t\")\n",
    "        words = ngram.split()\n",
    "        \n",
    "        for word in words:\n",
    "            #2.7 version: {coWord:int(count) for coWord in words if coWord != word}\n",
    "            stripe = dict((coWord, int(count)) for coWord in words if coWord !=word)\n",
    "            yield word, stripe\n",
    "                \n",
    "    def combiner(self,word, lines):\n",
    "        #stripe = dict(reduce(lambda x,y: self.combine_dicts(x,y), line))\n",
    "        stripe = reduce(lambda x,y: dict(x.items()+y.items()+ [(k, x[k] + y[k]) for k in set(x) & set(y)]), lines)\n",
    "        yield word, stripe\n",
    "    \n",
    "    def reducer(self,word, lines):\n",
    "        #stripe = dict(reduce(lambda x,y: Counter(x)+Counter(y), line))\n",
    "        stripe = reduce(lambda x,y: dict(x.items()+y.items()+ [(k, x[k] + y[k]) for k in set(x) & set(y)]), lines)\n",
    "        yield word, stripe\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer)]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    BuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.ask.20161004.023307.370251\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023307.370251/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob7634147654139689968.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0326\n",
      "  Submitted application application_1473978660783_0326\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0326/\n",
      "  Running job: job_1473978660783_0326\n",
      "  Job job_1473978660783_0326 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0326 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023307.370251/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148\n",
      "\t\tFILE: Number of bytes written=389733\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=463\n",
      "\t\tHDFS: Number of bytes written=163\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20990976\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19653120\n",
      "\t\tTotal time spent by all map tasks (ms)=13666\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40998\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7677\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38385\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13666\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7677\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4100\n",
      "\t\tCombine input records=7\n",
      "\t\tCombine output records=6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=190\n",
      "\t\tMap output materialized bytes=168\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1728794624\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=168\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=2478833664\n",
      "\t\tVirtual memory (bytes) snapshot=7760158720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023307.370251/output...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023307.370251...\n",
      "Removing temp directory /tmp/buildStripes.ask.20161004.023307.370251...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.ask.20161004.023407.516563\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023407.516563/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob1550155385605130227.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0327\n",
      "  Submitted application application_1473978660783_0327\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0327/\n",
      "  Running job: job_1473978660783_0327\n",
      "  Job job_1473978660783_0327 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0327 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023407.516563/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=561\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2402\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1045\n",
      "\t\tFILE: Number of bytes written=391669\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=997\n",
      "\t\tHDFS: Number of bytes written=2402\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20934144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10298880\n",
      "\t\tTotal time spent by all map tasks (ms)=13629\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40887\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4023\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20115\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13629\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4023\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3920\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=72\n",
      "\t\tInput split bytes=436\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=2970\n",
      "\t\tMap output materialized bytes=1096\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1745956864\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1096\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=2548039680\n",
      "\t\tVirtual memory (bytes) snapshot=7784456192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023407.516563/output...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/buildStripes.ask.20161004.023407.516563...\n",
      "Removing temp directory /tmp/buildStripes.ask.20161004.023407.516563...\n"
     ]
    }
   ],
   "source": [
    "!./buildStripes.py atlas-boon-systems-test.txt -r hadoop > atlasMiniStripesOutput.txt\n",
    "!./buildStripes.py googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt -r hadoop > goog10lineStripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\n",
      "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\n",
      "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\n",
      "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}\n",
      "\n",
      "\n",
      "\"A\"\t{\"City\": 62, \"Tales\": 123, \"Forms\": 116, \"in\": 1201, \"Wales\": 1099, \"ESTABLISHING\": 59, \"Christmas\": 1099, \"Government\": 102, \"Collection\": 239, \"RELIGIOUS\": 59, \"Case\": 604, \"Circumstantial\": 62, \"Female\": 447, \"FOR\": 59, \"Study\": 604, \"Narrative\": 62, \"Fairy\": 123, \"by\": 62, \"Limited\": 55, \"Childs\": 1099, \"of\": 895, \"BILL\": 59, \"General\": 92, \"Sea\": 62, \"the\": 124, \"George\": 92, \"Biography\": 92}\n",
      "\"BILL\"\t{\"A\": 59, \"RELIGIOUS\": 59, \"FOR\": 59, \"ESTABLISHING\": 59}\n",
      "\"Biography\"\t{\"A\": 92, \"of\": 92, \"George\": 92, \"General\": 92}\n",
      "\"by\"\t{\"A\": 62, \"City\": 62, \"the\": 62, \"Sea\": 62}\n",
      "\"Case\"\t{\"A\": 604, \"Limited\": 55, \"Government\": 102, \"of\": 502, \"Study\": 604, \"Female\": 447, \"in\": 102}\n",
      "\"Childs\"\t{\"A\": 1099, \"Wales\": 1099, \"Christmas\": 1099, \"in\": 1099}\n",
      "\"Christmas\"\t{\"A\": 1099, \"Wales\": 1099, \"Childs\": 1099, \"in\": 1099}\n",
      "\"Circumstantial\"\t{\"A\": 62, \"of\": 62, \"the\": 62, \"Narrative\": 62}\n",
      "\"City\"\t{\"A\": 62, \"the\": 62, \"by\": 62, \"Sea\": 62}\n",
      "\"Collection\"\t{\"A\": 239, \"of\": 239, \"Fairy\": 123, \"Tales\": 123, \"Forms\": 116}\n",
      "\"ESTABLISHING\"\t{\"A\": 59, \"BILL\": 59, \"RELIGIOUS\": 59, \"FOR\": 59}\n",
      "\"Fairy\"\t{\"A\": 123, \"of\": 123, \"Tales\": 123, \"Collection\": 123}\n",
      "\"Female\"\t{\"A\": 447, \"Case\": 447, \"Study\": 447, \"of\": 447}\n",
      "\"FOR\"\t{\"A\": 59, \"BILL\": 59, \"RELIGIOUS\": 59, \"ESTABLISHING\": 59}\n",
      "\"Forms\"\t{\"A\": 116, \"of\": 116, \"Collection\": 116}\n",
      "\"General\"\t{\"A\": 92, \"of\": 92, \"George\": 92, \"Biography\": 92}\n",
      "\"George\"\t{\"A\": 92, \"of\": 92, \"Biography\": 92, \"General\": 92}\n",
      "\"Government\"\t{\"A\": 102, \"Case\": 102, \"Study\": 102, \"in\": 102}\n",
      "\"in\"\t{\"A\": 1201, \"Case\": 102, \"Childs\": 1099, \"Government\": 102, \"Study\": 102, \"Wales\": 1099, \"Christmas\": 1099}\n",
      "\"Limited\"\t{\"A\": 55, \"Case\": 55, \"Study\": 55, \"of\": 55}\n",
      "\"Narrative\"\t{\"A\": 62, \"of\": 62, \"the\": 62, \"Circumstantial\": 62}\n",
      "\"of\"\t{\"A\": 1011, \"Case\": 502, \"Circumstantial\": 62, \"Limited\": 55, \"Study\": 502, \"Tales\": 123, \"General\": 92, \"Forms\": 232, \"Collection\": 355, \"Female\": 447, \"Narrative\": 62, \"the\": 62, \"Fairy\": 123, \"George\": 92, \"Biography\": 92}\n",
      "\"RELIGIOUS\"\t{\"A\": 59, \"BILL\": 59, \"FOR\": 59, \"ESTABLISHING\": 59}\n",
      "\"Sea\"\t{\"A\": 62, \"City\": 62, \"the\": 62, \"by\": 62}\n",
      "\"Study\"\t{\"A\": 604, \"Case\": 604, \"Government\": 102, \"of\": 502, \"Limited\": 55, \"Female\": 447, \"in\": 102}\n",
      "\"Tales\"\t{\"A\": 123, \"of\": 123, \"Fairy\": 123, \"Collection\": 123}\n",
      "\"the\"\t{\"A\": 124, \"City\": 62, \"Circumstantial\": 62, \"of\": 62, \"Sea\": 62, \"Narrative\": 62, \"by\": 62}\n",
      "\"Wales\"\t{\"A\": 1099, \"Childs\": 1099, \"Christmas\": 1099, \"in\": 1099}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat atlasMiniStripesOutput.txt | sort -k1,1\n",
    "printf \"\\n\\n\"\n",
    "cat goog10lineStripes.txt | sort -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 332aws: command not found\n",
      "/bin/sh: 32python: command not found\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###############################################\n",
    "\n",
    "!aws s3 rm --recursive s3://ucb261-hw5/hw5-4-stripes-mj\n",
    "!python buildStripes.py -r emr mini_stripes.txt \\\n",
    "    --cluster-id=j-1YW75NSU09AII \\\n",
    "    --output-dir=s3://ucb261-hw5/hw5-4-stripes-mj \\\n",
    "    --file=stopwords.txt \\\n",
    "    --file=mostFrequent/part-00000 \\\n",
    "# Output suppressed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10  Build an cooccureence strips from the atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the atlas-boon systems test\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripe documents for  atlas-boon systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Make Stripes from ngrams \n",
    "###############################################\n",
    "!aws s3 rm --recursive s3://ucb261-hw5/hw5-4-stripes-mj\n",
    "!python buildStripes.py -r emr mj_systems_test.txt \\\n",
    "    --cluster-id=j-2WHMJSLZDG \\\n",
    "    --output-dir=s3://ucb261-hw5/hw5-4-stripes-mj \\\n",
    "    --file=stopwords.txt \\\n",
    "    --file=mostFrequent/part-00000 \\\n",
    "# Output suppressed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir stripes-mj\n",
    "!aws s3 sync s3://ucb261-hw5/hw5-4-stripes-mj/  stripes-mj/\n",
    "!cat stripes-mj/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\n",
    "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\n",
    "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\n",
    "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building stripes execution MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 20  create inverted index, and calculate pairwise similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><strong>Solution 1:</strong> </p>\n",
    "<ol>\n",
    "<li>Create an Inverted Index. </li>\n",
    "<li>Use the output to calculate similarities. </li>\n",
    "<li>Build custom partitioner, re-run the similarity code, and output total order sorted partitions.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "#from collections import Counter\n",
    "import json\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "#Goal: Take in key {strip} file, and output inversion of {word: {doc1: wordsInDoc1, doc2: etc}, etc}\n",
    "class InvertedIndex(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        doc, stripe = line.strip(\"\\n\").split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "        stripeLength = len(stripe)\n",
    "        \n",
    "        for word in stripe.keys():\n",
    "            yield word, {doc.strip('\"'): stripeLength}\n",
    "                \n",
    "    def combiner(self,word, lines):\n",
    "        #A bit overkill because keys won't appear twice, but still combines it\n",
    "        #stripe = dict(reduce(lambda x,y: Counter(x)+Counter(y), line))\n",
    "        stripe = reduce(lambda x,y: dict(x.items()+y.items()+ [(k, x[k] + y[k]) for k in set(x) & set(y)]), lines)\n",
    "        yield word, stripe\n",
    "    \n",
    "    def reducer(self,word, lines):\n",
    "        #stripe = dict(reduce(lambda x,y: Counter(x)+Counter(y), line))\n",
    "        stripe = reduce(lambda x,y: dict(x.items()+y.items()+ [(k, x[k] + y[k]) for k in set(x) & set(y)]), lines)\n",
    "        yield word, stripe\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer)]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    InvertedIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.ask.20161004.024521.843469\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/invertedIndex.ask.20161004.024521.843469/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob9131051660422905421.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0331\n",
      "  Submitted application application_1473978660783_0331\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0331/\n",
      "  Running job: job_1473978660783_0331\n",
      "  Job job_1473978660783_0331 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0331 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/invertedIndex\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=245\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=153\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148\n",
      "\t\tFILE: Number of bytes written=389694\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=607\n",
      "\t\tHDFS: Number of bytes written=153\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19872768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18352640\n",
      "\t\tTotal time spent by all map tasks (ms)=12938\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38814\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7169\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=35845\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12938\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7169\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3530\n",
      "\t\tCombine input records=10\n",
      "\t\tCombine output records=8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=84\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=206\n",
      "\t\tMap output materialized bytes=186\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1734430720\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=186\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=2548039680\n",
      "\t\tVirtual memory (bytes) snapshot=7771987968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/invertedIndex...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/invertedIndex.ask.20161004.024521.843469...\n",
      "Removing temp directory /tmp/invertedIndex.ask.20161004.024521.843469...\n"
     ]
    }
   ],
   "source": [
    "!./invertedIndex.py mini_stripes.txt -r hadoop > stripesInvertedOutput.txt\n",
    "!./invertedIndex.py atlasMiniStripesOutput.txt -r hadoop --output-dir hdfs:///user/ask/tmp/mrjob/invertedIndex > atlasInvertedOutput.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"M\"\t{\"DocC\": 4}\n",
      "\"N\"\t{\"DocC\": 4}\n",
      "\"X\"\t{\"DocB\": 2, \"DocA\": 3}\n",
      "\"Y\"\t{\"DocB\": 2, \"DocC\": 4, \"DocA\": 3}\n",
      "\"Z\"\t{\"DocC\": 4, \"DocA\": 3}\n",
      "\n",
      "\n",
      "\"atlas\"\t{\"dipped\": 3, \"boon\": 3}\n",
      "\"boon\"\t{\"atlas\": 2, \"dipped\": 3, \"cava\": 2}\n",
      "\"cava\"\t{\"dipped\": 3, \"boon\": 3}\n",
      "\"dipped\"\t{\"atlas\": 2, \"boon\": 3, \"cava\": 2}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat stripesInvertedOutput.txt | sort -k1,1\n",
    "printf \"\\n\\n\"\n",
    "hdfs dfs -cat hdfs:///user/ask/tmp/mrjob/invertedIndex/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Systems test mini_stripes - Inverted Index\n",
    "\n",
    "            \"M\" |         DocC 4 |                |               \n",
    "            \"N\" |         DocC 4 |                |               \n",
    "            \"X\" |         DocA 3 |         DocB 2 |               \n",
    "            \"Y\" |         DocA 3 |         DocB 2 |         DocC 4\n",
    "            \"Z\" |         DocA 3 |         DocC 4 |               \n",
    "\n",
    " systems test atlas-boon - Inverted Index\n",
    "\n",
    "        \"atlas\" |         boon 3 |       dipped 3 |               \n",
    "       \"dipped\" |        atlas 2 |         boon 3 |         cava 2\n",
    "         \"boon\" |        atlas 2 |         cava 2 |       dipped 3\n",
    "         \"cava\" |         boon 3 |       dipped 3 |        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairwiseSimilarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairwiseSimilarity.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "#from collections import Counter\n",
    "import json\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "#Goal: Take in key {strip} file, and output inversion of {word: {doc1: wordsInDoc1, doc2: etc}, etc}\n",
    "class PairwiseSimilarity(MRJob):\n",
    "    \n",
    "    #For future reference, if this is too large to store in memory\n",
    "    #we can hack it.  Tack the union sum onto the end of the sorted key\n",
    "    #and then parse it all out at the reducer stage\n",
    "    #unions = defaultdict(int)\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(PairwiseSimilarity, self).configure_options()\n",
    "        self.add_passthrough_option(\"--similarity_measure\", type=\"str\")\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(PairwiseSimilarity, self).__init__(*args, **kwargs)\n",
    "        self.similarity_measure = self.options.similarity_measure\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        doc, stripe = line.strip(\"\\n\").split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "        stripeLength = len(stripe)\n",
    "        \n",
    "        if self.similarity_measure == \"jaccard\":\n",
    "            pairs = map(dict, itertools.combinations(stripe.items(),2))\n",
    "            for pair in pairs:\n",
    "                #A hack for sure, but pretty efficient way of storing (A+B) value\n",
    "                key = sorted(pair.keys()) + [str(sum(pair.values()))] # \",\".join(sorted(pair.keys()))\n",
    "                #self.unions[\",\".join(key)]=sum(pair.values())\n",
    "                yield key, 1\n",
    "        if self.similarity_measure == \"cosine\":\n",
    "            pairs = map(dict, itertools.combinations(stripe.items(),2))\n",
    "            for pair in pairs:\n",
    "                key = sorted(pair.keys()) # \",\".join(sorted(pair.keys()))\n",
    "                normProduct = reduce(lambda x,y: math.sqrt(x)*math.sqrt(y), pair.values())\n",
    "                yield key, float(1)/normProduct\n",
    "                \n",
    "    def combiner(self,key, values):\n",
    "        \n",
    "        if self.similarity_measure == \"jaccard\":\n",
    "            yield key, sum(values) \n",
    "        if self.similarity_measure == \"cosine\":\n",
    "            yield key, sum(values)\n",
    "    \n",
    "    def reducer(self,key, values):\n",
    "        totalCount = sum(values)\n",
    "        if self.similarity_measure == \"jaccard\":\n",
    "            #similarity = float(totalCount)/(self.unions[\",\".join(key)] - totalCount) #float(counts + singleCount)/union\n",
    "            similarity = float(totalCount)/(int(key[len(key)-1])-totalCount)\n",
    "            yield key[:-1], similarity\n",
    "        if self.similarity_measure == \"cosine\":\n",
    "            yield key, totalCount\n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer)]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    PairwiseSimilarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/tmp/mrjob/jaccardSimilarityStripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/.Trash/Current\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/tmp/mrjob/jaccardSimilarityAtlas' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/.Trash/Current\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/tmp/mrjob/cosineSimilarityStripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/.Trash/Current\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/tmp/mrjob/cosineSimilarityAtlas' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/ask/.Trash/Current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/10/04 03:44:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "16/10/04 03:44:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "16/10/04 03:44:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "16/10/04 03:44:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/pairwiseSimilarity.ask.20161004.034447.901122\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034447.901122/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob8489641504725742433.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0347\n",
      "  Submitted application application_1473978660783_0347\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0347/\n",
      "  Running job: job_1473978660783_0347\n",
      "  Job job_1473978660783_0347 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0347 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/jaccardSimilarityStripes\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=76\n",
      "\t\tFILE: Number of bytes written=390039\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20143104\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17861120\n",
      "\t\tTotal time spent by all map tasks (ms)=13114\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39342\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6977\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34885\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13114\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6977\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3440\n",
      "\t\tCombine input records=5\n",
      "\t\tCombine output records=4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tInput split bytes=370\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=120\n",
      "\t\tMap output materialized bytes=112\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1770520576\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=112\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=8\n",
      "\t\tTotal committed heap usage (bytes)=3649044480\n",
      "\t\tVirtual memory (bytes) snapshot=7767248896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/jaccardSimilarityStripes...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034447.901122...\n",
      "Removing temp directory /tmp/pairwiseSimilarity.ask.20161004.034447.901122...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/pairwiseSimilarity.ask.20161004.034546.458287\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034546.458287/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob5888652625415585720.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0349\n",
      "  Submitted application application_1473978660783_0349\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0349/\n",
      "  Running job: job_1473978660783_0349\n",
      "  Job job_1473978660783_0349 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0349 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/cosineSimilarityStripes\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=109\n",
      "\t\tFILE: Number of bytes written=390130\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19998720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17881600\n",
      "\t\tTotal time spent by all map tasks (ms)=13020\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39060\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6985\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34925\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13020\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6985\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3830\n",
      "\t\tCombine input records=5\n",
      "\t\tCombine output records=4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tInput split bytes=370\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=185\n",
      "\t\tMap output materialized bytes=157\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1744343040\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=157\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=8\n",
      "\t\tTotal committed heap usage (bytes)=2542796800\n",
      "\t\tVirtual memory (bytes) snapshot=7745228800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/cosineSimilarityStripes...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034546.458287...\n",
      "Removing temp directory /tmp/pairwiseSimilarity.ask.20161004.034546.458287...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/pairwiseSimilarity.ask.20161004.034644.857016\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034644.857016/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob3007021722147007620.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0351\n",
      "  Submitted application application_1473978660783_0351\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0351/\n",
      "  Running job: job_1473978660783_0351\n",
      "  Job job_1473978660783_0351 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0351 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/jaccardSimilarityAtlas\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=230\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=139\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=117\n",
      "\t\tFILE: Number of bytes written=390160\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=596\n",
      "\t\tHDFS: Number of bytes written=139\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20328960\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17635840\n",
      "\t\tTotal time spent by all map tasks (ms)=13235\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39705\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6889\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34445\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13235\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6889\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4460\n",
      "\t\tCombine input records=8\n",
      "\t\tCombine output records=8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=144\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=204\n",
      "\t\tMap output materialized bytes=179\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1697161216\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=179\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=2478833664\n",
      "\t\tVirtual memory (bytes) snapshot=7771037696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/jaccardSimilarityAtlas...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034644.857016...\n",
      "Removing temp directory /tmp/pairwiseSimilarity.ask.20161004.034644.857016...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/pairwiseSimilarity.ask.20161004.034743.709730\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034743.709730/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar] /tmp/streamjob6958873963306641100.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1473978660783_0352\n",
      "  Submitted application application_1473978660783_0352\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1473978660783_0352/\n",
      "  Running job: job_1473978660783_0352\n",
      "  Job job_1473978660783_0352 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1473978660783_0352 completed successfully\n",
      "  Output directory: hdfs:///user/ask/tmp/mrjob/cosineSimilarityAtlas\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=230\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=231\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149\n",
      "\t\tFILE: Number of bytes written=390226\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=596\n",
      "\t\tHDFS: Number of bytes written=231\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11844096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17548800\n",
      "\t\tTotal time spent by all map tasks (ms)=7711\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23133\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6855\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34275\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7711\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6855\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4090\n",
      "\t\tCombine input records=8\n",
      "\t\tCombine output records=8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=75\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=308\n",
      "\t\tMap output materialized bytes=225\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1716875264\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=225\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=2481455104\n",
      "\t\tVirtual memory (bytes) snapshot=7745163264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/ask/tmp/mrjob/cosineSimilarityAtlas...\n",
      "Removing HDFS temp directory hdfs:///user/ask/tmp/mrjob/pairwiseSimilarity.ask.20161004.034743.709730...\n",
      "Removing temp directory /tmp/pairwiseSimilarity.ask.20161004.034743.709730...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r hdfs:///user/ask/tmp/mrjob/jaccardSimilarityStripes\n",
    "hdfs dfs -rm -r hdfs:///user/ask/tmp/mrjob/jaccardSimilarityAtlas\n",
    "hdfs dfs -rm -r hdfs:///user/ask/tmp/mrjob/cosineSimilarityStripes\n",
    "hdfs dfs -rm -r hdfs:///user/ask/tmp/mrjob/cosineSimilarityAtlas\n",
    "\n",
    "./pairwiseSimilarity.py stripesInvertedOutput.txt -r hadoop --output-dir hdfs:///user/ask/tmp/mrjob/jaccardSimilarityStripes  --similarity_measure \"jaccard\" > jaccardSimilarity.txt\n",
    "./pairwiseSimilarity.py stripesInvertedOutput.txt -r hadoop --output-dir hdfs:///user/ask/tmp/mrjob/cosineSimilarityStripes --similarity_measure \"cosine\" > cosineSimilarity.txt\n",
    "\n",
    "./pairwiseSimilarity.py atlasInvertedOutput.txt -r hadoop --output-dir hdfs:///user/ask/tmp/mrjob/jaccardSimilarityAtlas --similarity_measure \"jaccard\" > jaccardAtlSimilarity.txt\n",
    "./pairwiseSimilarity.py atlasInvertedOutput.txt -r hadoop --output-dir hdfs:///user/ask/tmp/mrjob/cosineSimilarityAtlas --similarity_measure \"cosine\" > cosineAtlSimilarity.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity Measure\n",
      "\n",
      "[\"atlas\", \"boon\"]\t0.25\n",
      "[\"atlas\", \"cava\"]\t1.0\n",
      "[\"atlas\", \"dipped\"]\t0.25\n",
      "[\"boon\", \"cava\"]\t0.25\n",
      "[\"boon\", \"dipped\"]\t0.5\n",
      "[\"cava\", \"dipped\"]\t0.25\n",
      "\n",
      "\n",
      "[\"DocA\", \"DocB\"]\t0.66666666666666663\n",
      "[\"DocA\", \"DocC\"]\t0.40000000000000002\n",
      "[\"DocB\", \"DocC\"]\t0.20000000000000001\n",
      "\n",
      "\n",
      "Cosine Similarity Measure\n",
      "\n",
      "[\"atlas\", \"boon\"]\t0.40824829046386296\n",
      "[\"atlas\", \"cava\"]\t0.99999999999999978\n",
      "[\"atlas\", \"dipped\"]\t0.40824829046386296\n",
      "[\"boon\", \"cava\"]\t0.40824829046386296\n",
      "[\"boon\", \"dipped\"]\t0.66666666666666674\n",
      "[\"cava\", \"dipped\"]\t0.40824829046386296\n",
      "\n",
      "\n",
      "[\"DocA\", \"DocB\"]\t0.81649658092772592\n",
      "[\"DocA\", \"DocC\"]\t0.57735026918962584\n",
      "[\"DocB\", \"DocC\"]\t0.35355339059327373\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "printf \"Jaccard Similarity Measure\\n\\n\"\n",
    "cat jaccardAtlSimilarity.txt\n",
    "printf \"\\n\\n\"\n",
    "cat jaccardSimilarity.txt\n",
    "printf \"\\n\\nCosine Similarity Measure\\n\\n\"\n",
    "cat cosineAtlSimilarity.txt\n",
    "printf \"\\n\\n\"\n",
    "cat cosineSimilarity.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocB\"]\t0.66666666666666663\n",
      "[\"DocA\", \"DocC\"]\t0.40000000000000002\n",
      "[\"DocB\", \"DocC\"]\t0.20000000000000001\n",
      "\n",
      "[\"atlas\", \"boon\"]\t0.25\n",
      "[\"atlas\", \"cava\"]\t1.0\n",
      "[\"atlas\", \"dipped\"]\t0.25\n",
      "[\"boon\", \"cava\"]\t0.25\n",
      "[\"boon\", \"dipped\"]\t0.5\n",
      "[\"cava\", \"dipped\"]\t0.25\n",
      "\n",
      "[\"DocA\", \"DocB\"]\t0.81649658092772592\n",
      "[\"DocA\", \"DocC\"]\t0.57735026918962584\n",
      "[\"DocB\", \"DocC\"]\t0.35355339059327373\n",
      "\n",
      "[\"atlas\", \"boon\"]\t0.40824829046386296\n",
      "[\"atlas\", \"cava\"]\t0.99999999999999978\n",
      "[\"atlas\", \"dipped\"]\t0.40824829046386296\n",
      "[\"boon\", \"cava\"]\t0.40824829046386296\n",
      "[\"boon\", \"dipped\"]\t0.66666666666666674\n",
      "[\"cava\", \"dipped\"]\t0.40824829046386296\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "hdfs dfs -cat hdfs:///user/ask/tmp/mrjob/jaccardSimilarityStripes/part*\n",
    "printf \"\\n\"\n",
    "hdfs dfs -cat hdfs:///user/ask/tmp/mrjob/jaccardSimilarityAtlas/part*\n",
    "printf \"\\n\"\n",
    "hdfs dfs -cat hdfs:///user/ask/tmp/mrjob/cosineSimilarityStripes/part*\n",
    "printf \"\\n\"\n",
    "hdfs dfs -cat hdfs:///user/ask/tmp/mrjob/cosineSimilarityAtlas/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Calculations By Hand \n",
    "\n",
    "Jaccard Scratch Notes: \n",
    "    \n",
    "    docA & docB = {x + y}\n",
    "    docA | docB = {x + y + z}\n",
    "    A&B/A|B = .6666\n",
    "\n",
    "    docA & docC = {z + y}\n",
    "    docA | docC = {x + y + z+ m + n}\n",
    "    A&C/A|C = .4\n",
    "\n",
    "    docB & docC = {y}\n",
    "    docB | docC = {y + x + z + n + m}\n",
    "    C&B/C|B = .2\n",
    "\n",
    "    So jaccard literally is just using binary counts.\n",
    "\n",
    "    For each line i can spit out the combinations of docs/words, and also the sum of their attached counts \n",
    "\n",
    "    In the reduce phase we effectively get a free look at A&B, and we include A+B in the tuple, so then its A&B/(A+B-A&B)\n",
    "    \n",
    "    For words/docs that have no co-occurrence I would need to keep a set in memory of every term/document, and then a set of combinations, and then i would need to take the set difference, and output those combinations as zero.  This is doable, but ill ignore for now.\n",
    "\n",
    "Cosine Notes:\n",
    "    \n",
    "    docA*docB = {1*1 + 1*1 + 1*0} = 2\n",
    "    |docA||docB| = 1/sqrt(2) * 1/sqrt(3) = 1/sqrt(6)\n",
    "    A&B/A|B = .81\n",
    "\n",
    "    docA*docC = {1*0 + 1*1 + 1*1 + 1*0 + 1*0} = 2\n",
    "    docA | docC = 1/sqrt(3) * 1/sqrt(4) = 1/sqrt(12)\n",
    "    A&C/A|C = .57\n",
    "\n",
    "    docB*docC = {1*0 + 1*1 + 1*0 + 1*0 + 1*0} = 1\n",
    "    docB | docC = 1/sqrt(2) * 1/sqrt(4) = 1/sqrt(8)\n",
    "    C&B/C|B = .35\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Systems test mini_stripes - Inverted Index\n",
    "\n",
    "            \"M\" |         DocC 4 |                |               \n",
    "            \"N\" |         DocC 4 |                |               \n",
    "            \"X\" |         DocA 3 |         DocB 2 |               \n",
    "            \"Y\" |         DocA 3 |         DocB 2 |         DocC 4\n",
    "            \"Z\" |         DocA 3 |         DocC 4 |               \n",
    "\n",
    " systems test atlas-boon - Inverted Index\n",
    "\n",
    "        \"atlas\" |         boon 3 |       dipped 3 |               \n",
    "       \"dipped\" |        atlas 2 |         boon 3 |         cava 2\n",
    "         \"boon\" |        atlas 2 |         cava 2 |       dipped 3\n",
    "         \"cava\" |         boon 3 |       dipped 3 |   \n",
    "        \n",
    "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\n",
    "\"DocB\"\t{\"X\":100, \"Y\":20}\n",
    "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Systems test mini_stripes - Similarity measures\n",
    "\n",
    "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "       0.741582 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
    "       0.488675 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
    "       0.276777 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "Systems test atlas-boon 2 - Similarity measures\n",
    "\n",
    "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "       1.000000 |   atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
    "       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.1  <a name=\"1.3\"></a> Run systems tests on the CLOUD  (PHASE 1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 on the cloud (AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2: Full-scale experiment on Google N-gram data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.2  Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngramEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngramEDA.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "class NgramEDA(MRJob):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(NgramEDA, self).configure_options()\n",
    "        self.add_passthrough_option(\"--feature_type\", type=\"str\")\n",
    "        self.add_passthrough_option(\"--topN\", type=\"int\")\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NgramEDA, self).__init__(*args, **kwargs)\n",
    "        self.feature_type = self.options.feature_type\n",
    "        self.topN = self.options.topN\n",
    "        self.ngram = [\"nada\" for i in range(self.topN)]\n",
    "        self.frequencies = [0 for i in range(self.topN)]\n",
    "        \n",
    "    def mapper(self, key, line):\n",
    "        title, count, pages, books = line.strip(\"\\n\").split(\"\\t\")\n",
    "        words = title.split()\n",
    "        numChar = len(title)\n",
    "        \n",
    "        if self.feature_type == \"length\":\n",
    "            yield None, numChar\n",
    "        if self.feature_type == \"frequency\":\n",
    "            for word in words:\n",
    "                yield word, int(count)\n",
    "        if self.feature_type == \"density\":\n",
    "            for word in words:\n",
    "                yield word, (int(count),int(pages))\n",
    "        if self.feature_type == \"distribution\":\n",
    "            yield str(numChar), 1    \n",
    "                    \n",
    "    def reducer(self, key, counts):\n",
    "        if self.feature_type == \"length\":\n",
    "            yield \"Max Length\", max(counts)\n",
    "        if self.feature_type == \"frequency\":\n",
    "            total = sum(counts)\n",
    "            ix = -1\n",
    "            for i in range(len(self.frequencies)):\n",
    "                if total > self.frequencies[i]:\n",
    "                    ix = i\n",
    "                else:\n",
    "                    break\n",
    "            if ix >= 0:\n",
    "                self.frequencies.insert(ix+1,total)\n",
    "                self.ngram.insert(ix+1,key)\n",
    "                self.frequencies = self.frequencies[1:(1+len(self.frequencies))]\n",
    "                self.ngram = self.ngram[1:(1+len(self.frequencies))]\n",
    "            #yield key, total\n",
    "        if self.feature_type == \"density\":\n",
    "            count, pages = map(sum,zip(*counts))\n",
    "            yield key, float(count)/pages\n",
    "        if self.feature_type == \"distribution\":\n",
    "            yield key, sum(counts)\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        if self.feature_type == \"frequency\":\n",
    "            self.frequencies.reverse()\n",
    "            self.ngram.reverse()\n",
    "            print \"The top 10000 pages are:\"\n",
    "            for i in range(self.topN):\n",
    "                yield self.ngram[i] , self.frequencies[i]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer, reducer_final=self.reducer_final)]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    NgramEDA.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ngramEDA.cloudera.20161002.171514.216981\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/ngramEDA.cloudera.20161002.171514.216981/output...\n",
      "Removing temp directory /tmp/ngramEDA.cloudera.20161002.171514.216981...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ngramEDA.cloudera.20161002.171514.696650\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/ngramEDA.cloudera.20161002.171514.696650/output...\n",
      "Removing temp directory /tmp/ngramEDA.cloudera.20161002.171514.696650...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ngramEDA.cloudera.20161002.171515.068401\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/ngramEDA.cloudera.20161002.171515.068401/output...\n",
      "Removing temp directory /tmp/ngramEDA.cloudera.20161002.171515.068401...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ngramEDA.cloudera.20161002.171515.479685\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/ngramEDA.cloudera.20161002.171515.479685/output...\n",
      "Removing temp directory /tmp/ngramEDA.cloudera.20161002.171515.479685...\n"
     ]
    }
   ],
   "source": [
    "!./ngramEDA.py google5gram0Top10.txt --feature_type \"length\" --topN 20 > top10Length.txt\n",
    "!./ngramEDA.py google5gram0Top10.txt --jobconf mapred.reduce.tasks=1 --feature_type \"frequency\" --topN 20 > top10Frequency.txt\n",
    "!./ngramEDA.py google5gram0Top10.txt --feature_type \"density\" --topN 20  > top10Density.txt\n",
    "!./ngramEDA.py google5gram0Top10.txt --feature_type \"distribution\" --topN 20  > top10Distribution.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4  <a name=\"1.4\"></a> Synonym detection over 2Gig of Data\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print ''*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print ''*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"1.5\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\", \"boon\"]\n",
      "[\"atlas\", \"cava\"]\n",
      "[\"atlas\", \"dipped\"]\n",
      "[\"boon\", \"cava\"]\n",
      "[\"boon\", \"dipped\"]\n",
      "[\"cava\", \"dipped\"]\n",
      "[\"DocA\", \"DocB\"]\n",
      "[\"DocA\", \"DocC\"]\n",
      "[\"DocB\", \"DocC\"]\n",
      "[\"atlas\", \"boon\"]\n",
      "[\"atlas\", \"cava\"]\n",
      "[\"atlas\", \"dipped\"]\n",
      "[\"boon\", \"cava\"]\n",
      "[\"boon\", \"dipped\"]\n",
      "[\"cava\", \"dipped\"]\n",
      "[\"DocA\", \"DocB\"]\n",
      "[\"DocA\", \"DocC\"]\n",
      "[\"DocB\", \"DocC\"]\n",
      "\n",
      "Number of Hits: 0 out of top 18\n",
      "Number of words without synonyms: 36\n",
      "\n",
      "Precision\tnan\n",
      "Recall\t\tnan\n",
      "F1\t\tnan\n",
      "\n",
      "Words without synonyms:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n",
      "[] [\n",
      "[] \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/py27/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/opt/anaconda2/envs/py27/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "''' Performance measures '''\n",
    "#Partial-Author: Anthony Spalvieri-Kruse\n",
    "#I modified this script and used it for my synonym analysis \n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import re \n",
    "\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "## For this part we can use one of three outputs. They are all the same, but were generated differently\n",
    "# 1. the top 1000 from the full sorted dataset -> sortedSims[:1000]\n",
    "# 2. the top 1000 from the partial sort aggragate file -> sims2/top1000sims\n",
    "# 3. the top 1000 from the total order sort file -> head -1000 sims_parts/part-00004\n",
    "\n",
    "f1 = open(\"jaccardAtlSimilarity.txt\",\"r\")\n",
    "f2 = open(\"jaccardSimilarity.txt\",\"r\")\n",
    "f3 = open(\"cosineAtlSimilarity.txt\",\"r\")\n",
    "f4 = open(\"cosineSimilarity.txt\", \"r\")\n",
    "\n",
    "f1 = f1.readlines()\n",
    "f2 = f2.readlines()\n",
    "f3 = f3.readlines()\n",
    "f4 = f4.readlines()\n",
    "\n",
    "f1 = [i.strip(\"\\n\").split(\"\\t\") for i in f1]\n",
    "f2 = [i.strip(\"\\n\").split(\"\\t\") for i in f2]\n",
    "f3 = [i.strip(\"\\n\").split(\"\\t\") for i in f3]\n",
    "f4 = [i.strip(\"\\n\").split(\"\\t\") for i in f4]\n",
    "\n",
    "top1000sims = f1+f2+f3+f4\n",
    "#with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "#    for line in f.readlines():\n",
    "#\n",
    "#        line = line.strip()\n",
    "#        avg,lisst = line.split(\"\\t\")\n",
    "#        lisst = json.loads(lisst)\n",
    "#        lisst.append(avg)\n",
    "#        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    print words\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
